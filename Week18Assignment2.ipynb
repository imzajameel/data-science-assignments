{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a457d3f-3218-40a6-b769-c8dd459fc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.18 \n",
    "#Assignment.2 \n",
    "#Question.1 : What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "#metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "#Answer.1 : # Euclidean Distance:\n",
    "\n",
    "# Formula:\n",
    "# Euclidean Distance (A, B) = sqrt(sum((A_i - B_i)^2))\n",
    "\n",
    "# Characteristics:\n",
    "# - Also known as L2 norm or straight-line distance.\n",
    "# - Measures the straight-line distance between two points in Euclidean space.\n",
    "# - Reflects the actual distance between points, considering both horizontal and vertical movements.\n",
    "\n",
    "# Manhattan Distance:\n",
    "\n",
    "# Formula:\n",
    "# Manhattan Distance (A, B) = sum(|A_i - B_i|)\n",
    "\n",
    "# Characteristics:\n",
    "# - Also known as L1 norm or city block distance.\n",
    "# - Measures the distance between two points by moving only horizontally and vertically (no diagonals).\n",
    "# - Represents the distance traveled along the grid lines of a city block.\n",
    "\n",
    "# Differences:\n",
    "\n",
    "# 1. Direction of Measurement:\n",
    "#    - Euclidean distance measures the straight-line distance, considering both horizontal and vertical movements.\n",
    "#    - Manhattan distance measures the distance by moving only horizontally and vertically along grid lines.\n",
    "\n",
    "# 2. Sensitivity to Magnitudes:\n",
    "#    - Euclidean distance is sensitive to magnitudes of individual features due to the squared term in the formula.\n",
    "#    - Manhattan distance treats magnitudes of features equally since it only considers absolute differences.\n",
    "\n",
    "# 3. Paths Considered:\n",
    "#    - Euclidean distance considers the shortest path (diagonal) between two points.\n",
    "#    - Manhattan distance considers paths along the grid lines (horizontal and vertical).\n",
    "\n",
    "# Impact on KNN Performance:\n",
    "\n",
    "# 1. Isotropy vs. Anisotropy:\n",
    "#    - Euclidean distance assumes isotropic (uniform) relationships between features.\n",
    "#    - Manhattan distance is useful when the relationships between features are anisotropic (vary in different directions).\n",
    "\n",
    "# 2. Outlier Sensitivity:\n",
    "#    - Euclidean distance is sensitive to outliers as it considers the shortest path.\n",
    "#    - Manhattan distance is less sensitive to outliers due to its path along grid lines.\n",
    "\n",
    "# 3. Feature Scaling:\n",
    "#    - Euclidean distance can be affected by differences in feature magnitudes.\n",
    "#    - Manhattan distance is less influenced by feature magnitudes.\n",
    "\n",
    "# 4. Performance Trade-off:\n",
    "#    - The choice between Euclidean and Manhattan distance may impact the performance of a KNN classifier or regressor.\n",
    "#    - Experimenting with both distance metrics and observing their impact on model performance is crucial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb46688a-b37b-4fb7-9697-3e6f1edb0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "#used to determine the optimal k value?\n",
    "#Answer.2 : # Choosing the Optimal Value of k for KNN:\n",
    "\n",
    "# 1. Cross-Validation:\n",
    "\n",
    "# Technique:\n",
    "# - Use k-fold cross-validation to evaluate the model's performance for different k values.\n",
    "# - Split the dataset into k folds, train the model on k-1 folds, and validate on the remaining fold.\n",
    "\n",
    "# Implementation:\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#k_values = [1, 3, 5, 7, 9, 11]  # Example values of k\n",
    "#for k in k_values:\n",
    "#    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "#    print(f'For k={k}, Average Accuracy: {scores.mean()}')\n",
    "\n",
    "\n",
    "# 2. Grid Search with Cross-Validation:\n",
    "\n",
    "# Technique:\n",
    "# - Perform a grid search over a range of k values along with other hyperparameters.\n",
    "# - Use cross-validation to evaluate the model for each combination of hyperparameters.\n",
    "\n",
    "# Implementation:\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11]}\n",
    "#knn = KNeighborsClassifier()\n",
    "#grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "#grid_search.fit(X, y)\n",
    "#print(f'Best k: {grid_search.best_params_[\"n_neighbors\"]}')\n",
    "\n",
    "\n",
    "# 3. Elbow Method:\n",
    "\n",
    "# Technique:\n",
    "# - Plot the model performance (e.g., accuracy or error) against different k values.\n",
    "# - Look for the point where the performance starts to plateau (the \"elbow\" point).\n",
    "\n",
    "# Implementation:\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#error_rates = []\n",
    "#k_values = range(1, 21)  # Example range of k\n",
    "#for k in k_values:\n",
    "#    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#    knn.fit(X_train, y_train)\n",
    " #   y_pred = knn.predict(X_val)\n",
    "#    error_rate = 1 - accuracy_score(y_val, y_pred)\n",
    "#    error_rates.append(error_rate)\n",
    "\n",
    "#plt.plot(k_values, error_rates, marker='o')\n",
    "#plt.xlabel('k')\n",
    "#plt.ylabel('Error Rate')\n",
    "#plt.title('Elbow Method for Optimal k')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# 4. Leave-One-Out Cross-Validation (LOOCV):\n",
    "\n",
    "# Technique:\n",
    "# - A special case of cross-validation where each data point is used as a validation set while the rest are used for\n",
    "#training.\n",
    "# - Evaluate the model for each k value.\n",
    "\n",
    "# Implementation:\n",
    "#from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "#loo = LeaveOneOut()\n",
    "#k_values = [1, 3, 5, 7, 9, 11]  # Example values of k\n",
    "#for k in k_values:\n",
    "#    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#    scores = cross_val_score(knn, X, y, cv=loo, scoring='accuracy')\n",
    "#    print(f'For k={k}, Average Accuracy: {scores.mean()}')\n",
    "\n",
    "\n",
    "# 5. Model Evaluation Metrics:\n",
    "\n",
    "# Technique:\n",
    "# - Consider using relevant evaluation metrics (e.g., accuracy, precision, recall, F1-score) to assess the model's\n",
    "#performance for different k values.\n",
    "\n",
    "# Implementation:\n",
    "#from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#k_values = [1, 3, 5, 7, 9, 11]  # Example values of k\n",
    "#for k in k_values:\n",
    "#    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#    knn.fit(X_train, y_train)\n",
    "#    y_pred = knn.predict(X_val)\n",
    "#    precision = precision_score(y_val, y_pred)\n",
    "#    recall = recall_score(y_val, y_pred)\n",
    "#    f1 = f1_score(y_val, y_pred)\n",
    "#    print(f'For k={k}, Precision: {precision}, Recall: {recall}, F1-score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e739a2b0-7979-4116-b60a-aa8a4b74c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "#what situations might you choose one distance metric over the other?\n",
    "#Answer.3 : # Euclidean Distance:\n",
    "\n",
    "# Characteristics:\n",
    "# - Measures the straight-line distance between two points in Euclidean space.\n",
    "# - Formula: Euclidean Distance (A, B) = sqrt(sum((A_i - B_i)^2))\n",
    "# - Assumes isotropic (uniform) relationships between features.\n",
    "\n",
    "# Performance Impact:\n",
    "# - Sensitive to differences in feature magnitudes.\n",
    "# - More influenced by outliers due to considering the shortest path.\n",
    "\n",
    "# Use Cases:\n",
    "# - Suitable when relationships between features are expected to be isotropic (uniform).\n",
    "# - Appropriate for datasets where the magnitudes of individual features are relevant.\n",
    "\n",
    "\n",
    "# Manhattan Distance:\n",
    "\n",
    "# Characteristics:\n",
    "# - Also known as L1 norm or city block distance.\n",
    "# - Measures the distance between two points by moving only horizontally and vertically (no diagonals).\n",
    "# - Formula: Manhattan Distance (A, B) = sum(|A_i - B_i|)\n",
    "# - Useful when relationships between features are anisotropic (vary in different directions).\n",
    "\n",
    "# Performance Impact:\n",
    "# - Less sensitive to differences in feature magnitudes compared to Euclidean distance.\n",
    "# - Less influenced by outliers due to considering paths along grid lines.\n",
    "\n",
    "# Use Cases:\n",
    "# - Suitable when movement can only occur along axes (e.g., urban navigation where movement is restricted to streets).\n",
    "# - Robust to outliers and situations where isotropic relationships between features may not hold.\n",
    "\n",
    "\n",
    "# Choosing Between Euclidean and Manhattan Distance:\n",
    "\n",
    "# Data Characteristics:\n",
    "# - If the dataset exhibits isotropic relationships and features have similar magnitudes, Euclidean distance may be suitable.\n",
    "# - If the dataset has anisotropic relationships or varying feature magnitudes, Manhattan distance might be more appropriate.\n",
    "\n",
    "# Feature Scaling:\n",
    "# - Euclidean distance can be sensitive to differences in feature magnitudes, so scaling features may be necessary.\n",
    "# - Manhattan distance is less influenced by feature magnitudes, making it more robust to unscaled features.\n",
    "\n",
    "# Outliers:\n",
    "# - If outliers are present and their impact needs to be minimized, Manhattan distance may be preferred.\n",
    "# - Euclidean distance can be more sensitive to outliers due to considering the shortest path.\n",
    "\n",
    "# Application Specifics:\n",
    "# - Consider the nature of the problem and domain knowledge.\n",
    "# - Choose the distance metric that aligns with the underlying characteristics of the data.\n",
    "\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "# The choice between Euclidean and Manhattan distance in KNN depends on the specific characteristics of the dataset\n",
    "# and the nature of relationships between features.\n",
    "# Experimentation with both distance metrics and observing their impact on model performance is recommended to make\n",
    "# an informed decision based on the specific requirements of the problem at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536aa7ab-110e-4b29-8a00-fb3fa8769aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "#the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "#model performance?\n",
    "#Answer.4 : # Common Hyperparameters in KNN:\n",
    "\n",
    "# 1. Number of Neighbors (k):\n",
    "#    - Effect: Determines the number of nearest neighbors considered when making predictions.\n",
    "#    - Tuning Strategy: Use cross-validation or grid search to find the optimal value for k based on model performance.\n",
    "\n",
    "# 2. Distance Metric:\n",
    "#    - Effect: Specifies the distance measure used to calculate the similarity between data points (e.g., Euclidean,\n",
    "#Manhattan).\n",
    "#    - Tuning Strategy: Experiment with different distance metrics based on the characteristics of the data. Grid search\n",
    "#can help identify the most suitable metric.\n",
    "\n",
    "# 3. Weight Function:\n",
    "#    - Effect: Defines how the contributions of neighbors are weighted when making predictions (e.g., uniform or\n",
    "#distance-based weights).\n",
    "#    - Tuning Strategy: Test different weight functions and choose the one that improves model performance. Grid \n",
    "#search can assist in finding the optimal weight function.\n",
    "\n",
    "# 4. Algorithm:\n",
    "#    - Effect: Determines the algorithm used to compute nearest neighbors (e.g., 'auto', 'ball_tree', 'kd_tree', 'brute').\n",
    "#    - Tuning Strategy: The choice of algorithm depends on the dataset size and characteristics. Experiment with \n",
    "#different algorithms and select the one that performs best.\n",
    "\n",
    "# Tuning Strategies:\n",
    "\n",
    "# 1. Grid Search:\n",
    "#    - Define a grid of hyperparameter values.\n",
    "#    - Perform cross-validation for each combination of hyperparameters.\n",
    "#    - Select the combination that maximizes performance.\n",
    "# Example:\n",
    "# param_grid = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': \n",
    "#['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "# grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X, y)\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# 2. Randomized Search:\n",
    "#    - Similar to grid search but randomly samples hyperparameter combinations.\n",
    "#    - Useful when the search space is large.\n",
    "# Example:\n",
    "# param_dist = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree',\n",
    "#'brute']}\n",
    "# random_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
    "# random_search.fit(X, y)\n",
    "# best_params = random_search.best_params_\n",
    "\n",
    "# 3. Manual Tuning:\n",
    "#    - Experiment with a range of hyperparameter values manually.\n",
    "#    - Use visualization tools or performance metrics to assess the impact of different hyperparameter choices.\n",
    "# Example:\n",
    "# k_values = [3, 5, 7]\n",
    "# weights_options = ['uniform', 'distance']\n",
    "# best_accuracy = 0\n",
    "# best_params = {}\n",
    "# for k in k_values:\n",
    "#     for weight in weights_options:\n",
    "#         knn = KNeighborsClassifier(n_neighbors=k, weights=weight)\n",
    "#         scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "#         avg_accuracy = scores.mean()\n",
    "#         if avg_accuracy > best_accuracy:\n",
    "#             best_accuracy = avg_accuracy\n",
    "#             best_params = {'n_neighbors': k, 'weights': weight}\n",
    "\n",
    "# 4. Iterative Tuning:\n",
    "#    - Gradually adjust hyperparameters based on observations from initial model runs.\n",
    "#    - Monitor changes in model performance and adjust hyperparameters accordingly.\n",
    "# Example:\n",
    "# Adjust hyperparameters based on performance observations during model development.\n",
    "\n",
    "# Conclusion:\n",
    "# Tuning hyperparameters in KNN involves finding the combination that optimizes model performance.\n",
    "# Techniques such as grid search, randomized search, manual tuning, and iterative tuning help identify\n",
    "# the most suitable hyperparameter values for a specific dataset.\n",
    "# It's essential to balance model complexity and avoid overfitting by selecting hyperparameters that generalize well\n",
    "#to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6890d88-50a0-4701-8b09-8cb591073647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "#techniques can be used to optimize the size of the training set?\n",
    "#Answer.5 : # Impact of Training Set Size:\n",
    "\n",
    "# 1. Small Training Set:\n",
    "#    - Effects:\n",
    "#      - Increased model variance.\n",
    "#      - Sensitivity to noise and outliers.\n",
    "#      - Overfitting to the training data.\n",
    "#    - Recommendations:\n",
    "#      - More likely to capture noise as the model memorizes the training set.\n",
    "#      - Higher risk of poor generalization to new, unseen data.\n",
    "\n",
    "# 2. Large Training Set:\n",
    "#    - Effects:\n",
    "#      - Improved generalization to new data.\n",
    "#      - Reduced model variance.\n",
    "#      - Robustness to noise and outliers.\n",
    "#    - Recommendations:\n",
    "#      - Provides a more reliable estimation of the underlying data distribution.\n",
    "#      - Reduces the risk of overfitting and results in a more robust model.\n",
    "\n",
    "# Techniques to Optimize Training Set Size:\n",
    "\n",
    "# 1. Cross-Validation:\n",
    "#    - Use techniques like k-fold cross-validation to assess model performance across different subsets of the data.\n",
    "#    - Helps evaluate the model's ability to generalize to new data, providing insights into potential overfitting.\n",
    "# Example:\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# scores = cross_val_score(knn, X_train, y_train, cv=kf)\n",
    "\n",
    "# 2. Learning Curves:\n",
    "#    - Plot learning curves to visualize how model performance changes with varying training set sizes.\n",
    "#    - Analyze convergence and assess the trade-off between bias and variance.\n",
    "# Example:\n",
    "# train_sizes, train_scores, test_scores = learning_curve(knn, X_train, y_train, cv=5)\n",
    "\n",
    "# 3. Incremental Learning:\n",
    "#    - For large datasets, consider incremental or online learning approaches where the model is updated\n",
    "#continuously with new data.\n",
    "#    - This can be beneficial for adapting to changes in the underlying data distribution.\n",
    "# Example:\n",
    "# incremental_model = SGDClassifier()\n",
    "# incremental_model.partial_fit(new_X, new_y, classes=np.unique(y))\n",
    "\n",
    "# 4. Feature Selection/Dimensionality Reduction:\n",
    "#    - If dimensionality is high, consider feature selection or dimensionality reduction techniques to focus on\n",
    "#the most informative features.\n",
    "#    - This can help mitigate the curse of dimensionality and make the model more efficient.\n",
    "# Example:\n",
    "# selector = SelectKBest(score_func=f_classif, k=10)\n",
    "# X_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# 5. Data Augmentation:\n",
    "#    - Increase the effective size of the training set by applying data augmentation techniques.\n",
    "#    - For image data, this could include random rotations, flips, or small perturbations.\n",
    "# Example:\n",
    "# datagen = ImageDataGenerator(rotation_range=10, horizontal_flip=True)\n",
    "\n",
    "# 6. Data Balancing:\n",
    "#    - For imbalanced datasets, use techniques to balance class distributions.\n",
    "#    - This can prevent the model from being biased towards the majority class.\n",
    "# Example:\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Conclusion:\n",
    "# The size of the training set is a crucial factor in the performance of a KNN classifier or regressor.\n",
    "# Balancing the training set size with the complexity of the problem, the dimensionality of the feature space,\n",
    "# and the potential for overfitting is essential.\n",
    "# Techniques such as cross-validation, learning curves, incremental learning, feature selection, data augmentation,\n",
    "# and data balancing can be employed to optimize the training set size and improve the model's generalization capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f8f15-b360-44a8-a265-b3263bc0caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "#overcome these drawbacks to improve the performance of the model?\n",
    "#Answer.6 : # Potential Drawbacks of KNN:\n",
    "\n",
    "# 1. Computational Complexity:\n",
    "#    - Drawback: KNN's prediction time increases with the size of the training set, making it computationally expensive\n",
    "#for large datasets.\n",
    "#    - Overcoming Strategy: Use approximate nearest neighbors algorithms, such as KD-trees or Ball trees, to speed up \n",
    "#the search process.\n",
    "#      Additionally, consider dimensionality reduction techniques to reduce computational costs.\n",
    "# Example:\n",
    "# from sklearn.neighbors import KDTree\n",
    "# kdtree = KDTree(X_train)\n",
    "\n",
    "# 2. Memory Usage:\n",
    "#    - Drawback: KNN requires storing the entire training dataset in memory for prediction, which can be\n",
    "#impractical for very large datasets.\n",
    "#    - Overcoming Strategy: Implement algorithms for approximate nearest neighbors or use techniques like locality-sensitive\n",
    "#hashing (LSH)  to reduce memory requirements.\n",
    "\n",
    "# Example:\n",
    "# from sklearn.neighbors import LSHForest\n",
    "# lshf = LSHForest(n_neighbors=5)\n",
    "# lshf.fit(X_train)\n",
    "\n",
    "# 3. Sensitivity to Irrelevant Features:\n",
    "#    - Drawback: KNN considers all features equally, making it sensitive to irrelevant or noisy features.\n",
    "#    - Overcoming Strategy: Conduct feature selection or dimensionality reduction to focus on the most informative features.\n",
    "#      This helps reduce the impact of irrelevant features on the model.\n",
    "# Example:\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# selector = SelectKBest(score_func=f_classif, k=10)\n",
    "# X_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# 4. Impact of Outliers:\n",
    "#    - Drawback: KNN can be sensitive to outliers as they may significantly affect distance calculations.\n",
    "#    - Overcoming Strategy: Preprocess the data to identify and handle outliers appropriately. Techniques like data \n",
    "#scaling and robust\n",
    "#      distance metrics can reduce the influence of outliers.\n",
    "# Example:\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# scaler = RobustScaler()\n",
    "# X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 5. Curse of Dimensionality:\n",
    "#    - Drawback: In high-dimensional spaces, the distance between points tends to become more uniform, reducing \n",
    "#the effectiveness of distance-based methods.\n",
    "#    - Overcoming Strategy: Apply dimensionality reduction techniques, feature selection, or feature engineering\n",
    "#to mitigate the curse of dimensionality.\n",
    "#      Consider algorithms that are less affected by high dimensions.\n",
    "# Example:\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=10)\n",
    "# X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# 6. Unequal Class Distributions (for Classification):\n",
    "#    - Drawback: KNN can be biased towards the majority class in imbalanced datasets.\n",
    "#    - Overcoming Strategy: Use techniques like oversampling the minority class (e.g., SMOTE) or adjusting class\n",
    "#weights to address imbalanced class distributions.\n",
    "# Example:\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Conclusion:\n",
    "# While KNN is a simple and intuitive algorithm, it has certain limitations that can impact its performance, especially\n",
    "#in scenarios involving\n",
    "# large datasets, high dimensionality, and noisy features. Employing strategies such as approximate nearest neighbors, \n",
    "#dimensionality reduction,\n",
    "# outlier handling, and addressing class imbalances can enhance the robustness and efficiency of KNN-based models.\n",
    "# It's essential to carefully preprocess the data, choose appropriate hyperparameters, and consider the specific \n",
    "#characteristics of the problem\n",
    "# when using KNN to achieve optimal results. Additionally, combining KNN with other algorithms in ensemble methods or \n",
    "#hybrid models may provide further improvements in performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
