{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083b771d-79f7-43a6-9071-883336010797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature_engineering_Assignment.2 \n",
    "#Question.1 : What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "#application.\n",
    "#Answer.1 : Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform \n",
    "#numerical features of a dataset so that they are scaled to a specific range, typically between 0 and 1. This \n",
    "#technique is particularly useful when the features have different scales and ranges, and you want to bring them \n",
    "#to a common scale for better performance of machine learning algorithms that are sensitive to the scale of the\n",
    "#features.\n",
    "\n",
    "#The formula for Min-Max scaling is as follows for each feature:\n",
    "\n",
    "#Xscaled = (X-Xmin)/(Xmax - Xmin)\n",
    " \n",
    "\n",
    "#Where:\n",
    "\n",
    "#X is the original value of the feature.\n",
    "\n",
    "#Xmin is the minimum value of the feature in the dataset.\n",
    "\n",
    "#Xmax is the maximum value of the feature in the dataset.\n",
    "\n",
    "#Xscaled is the scaled value of the feature between 0 and 1.\n",
    "\n",
    "#Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "#Suppose you have a dataset with a feature \"Age\" representing people's ages and a feature \"Income\" \n",
    "#representing their income in dollars. The original values are as follows:\n",
    "\n",
    "#|   Age   |  Income  |\n",
    "#|---------|----------|\n",
    "#|   25    |  40000   |\n",
    "#|   40    |  80000   |\n",
    "#|   30    |  60000   |\n",
    "#|   22    |  30000   |\n",
    "\n",
    "#To apply Min-Max scaling, you calculate the minimum and maximum values for each feature:\n",
    "\n",
    "#For \"Age\":\n",
    "\n",
    "#Xmin, Age = 22\n",
    "\n",
    "#Xmax, Age = 40\n",
    "\n",
    "#For \"Income\":\n",
    "\n",
    "#Xmin, Income = 30000\n",
    "\n",
    "#Xmax, Income  = 80000\n",
    "\n",
    "#Now you can apply the Min-Max scaling formula to each data point:\n",
    "\n",
    "#For the first row :\n",
    "\n",
    "#Age : Xscaled,age = (25-22)/(40-22) = 3/18 = 0.167\n",
    "\n",
    "#Income : Xscaled,Income = (40000-30000)/(80000-30000) = (10000)/(50000) = 0.2\n",
    "\n",
    "#You repeat this process for each data point. After scaling all the data points, your dataset will have values \n",
    "#scaled between 0 and 1:\n",
    "\n",
    "#|   Age   |  Income  |\n",
    "#|---------|----------|\n",
    "#|  0.167  |   0.2    |\n",
    "#|  0.667  |   0.8    |\n",
    "#|  0.333  |   0.5    |\n",
    "#|  0.000  |   0.0    |\n",
    "\n",
    "#Now both \"Age\" and \"Income\" are on the same scale, which can help improve the performance of machine \n",
    "#learning algorithms that are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e51853d-6dc8-4560-827b-40e6e389bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "#Provide an example to illustrate its application.\n",
    "#Answer.2 :The Unit Vector technique, also known as vector normalization or feature scaling, is a method\n",
    "#used to scale individual data samples in a dataset to have a Euclidean norm (magnitude) of 1. This technique is \n",
    "#particularly useful when the direction of the data is more important than its magnitude, such as in algorithms\n",
    "#like k-nearest neighbors (KNN) or when dealing with text data and cosine similarity.\n",
    "\n",
    "#The formula for Unit Vector scaling is as follows for each data sample:\n",
    "\n",
    "#X_scaled = X / np.linalg.norm(X)\n",
    "\n",
    "#Example:\n",
    "\n",
    "#Suppose you have a dataset with two features, \"Height\" and \"Weight,\" representing people's physical characteristics.\n",
    "#The original values are as follows:\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#data = np.array([\n",
    "   # [160, 60],\n",
    "   # [180, 75],\n",
    "   # [170, 65]\n",
    "#])\n",
    "#To apply the Unit Vector technique, you can use the following code:\n",
    "\n",
    "#euclidean_norms = np.linalg.norm(data, axis=1)\n",
    "#scaled_data = data / euclidean_norms[:, np.newaxis]\n",
    "#The euclidean_norms array contains the Euclidean norm for each data sample, and scaled_data contains the scaled unit vectors.\n",
    "\n",
    "#Printing the original data and scaled unit vectors:\n",
    "\n",
    "#print(\"Original data:\")\n",
    "#print(data)\n",
    "#print(\"\\nScaled unit vectors:\")\n",
    "#print(scaled_data)\n",
    "#When you run this code, you'll see the original data and the scaled unit vectors for the dataset. The scaled unit\n",
    "#vectors have a Euclidean norm of 1 while preserving the direction of the original data.\n",
    "\n",
    "#Difference from Min-Max Scaling:\n",
    "\n",
    "#The key difference between Unit Vector scaling and Min-Max scaling lies in their goals and effects. Unit Vector\n",
    "#scaling focuses on preserving the direction of data vectors while making their magnitudes uniform. Min-Max scaling, \n",
    "#on the other hand, aims to bring data features within a specific range (typically 0 to 1) while maintaining their \n",
    "#relative differences.\n",
    "\n",
    "#In summary, Unit Vector scaling is ideal when preserving data direction matters, while Min-Max scaling is useful \n",
    "#when you want to ensure that all features are on a common scale, regardless of their original ranges. The choice \n",
    "#between these techniques depends on the characteristics of the data and the requirements of the machine learning \n",
    "#algorithm being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f12e488-98de-4d95-aa53-d09ee8826e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qustion.3 :  What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "#example to illustrate its application.\n",
    "#Answer.3 : Principal Component Analysis (PCA) is a widely used technique in dimensionality reduction and data \n",
    "#compression. It aims to transform high-dimensional data into a lower-dimensional space while preserving as much \n",
    "#of the variance (information) as possible. This is achieved by identifying the principal components, which are\n",
    "#orthogonal linear combinations of the original features that capture the most significant patterns in the data.\n",
    "\n",
    "#Here's how PCA works in a nutshell:\n",
    "\n",
    "#Center the data: Subtract the mean from each feature to make the data's center at the origin.\n",
    "\n",
    "#Compute the covariance matrix: Calculate the covariance matrix of the centered data. This matrix shows how the\n",
    "#features vary together.\n",
    "\n",
    "#Compute eigenvectors and eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors\n",
    "#represent the directions of maximum variance, and eigenvalues indicate the magnitude of variance along those directions.\n",
    "\n",
    "#Sort eigenvectors by eigenvalues: Arrange the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "#This determines the importance of each principal component.\n",
    "\n",
    "#Select top k eigenvectors: Choose the first k eigenvectors with the highest eigenvalues to form the new lower-dimensional\n",
    "#space. These eigenvectors are the principal components.\n",
    "\n",
    "#Project data onto new space: Multiply the original data by the selected eigenvectors to obtain the transformed data in \n",
    "#the lower-dimensional space.\n",
    "\n",
    "#Example:\n",
    "\n",
    "#Let's say you have a dataset with two features, \"Height\" and \"Weight,\" and you want to perform PCA for dimensionality\n",
    "#reduction.\n",
    "\n",
    "#Original data:\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#data = np.array([\n",
    "    #[160, 60],\n",
    "    #[180, 75],\n",
    "    #[170, 65]\n",
    "#])\n",
    "\n",
    "#Applying PCA:\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with 1 principal component\n",
    "#pca = PCA(n_components=1)\n",
    "\n",
    "# Fit and transform the data\n",
    "#data_transformed = pca.fit_transform(data)\n",
    "\n",
    "#print(\"Original data:\")\n",
    "#print(data)\n",
    "#print(\"\\nTransformed data:\")\n",
    "#print(data_transformed)\n",
    "\n",
    "#In this example, we are using the PCA class from the sklearn.decomposition module to perform PCA. We specify that\n",
    "#we want to reduce the data to 1 dimension (n_components=1).\n",
    "\n",
    "#After transforming the data, you'll see that the \"Transformed data\" contains the values projected onto the first\n",
    "#principal component, effectively reducing the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "#PCA can be a powerful tool for reducing the dimensionality of data, which can lead to faster and more efficient \n",
    "#computations and can also help in visualization and noise reduction while retaining the most important patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800e5e2d-adc8-4323-ad4a-411a014ba289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "#Extraction? Provide an example to illustrate this concept.\n",
    "#Answer.4 : PCA (Principal Component Analysis) is a dimensionality reduction technique that can also be used for \n",
    "#feature extraction. While these terms might seem similar, they have distinct roles:\n",
    "\n",
    "#PCA and Dimensionality Reduction: PCA is primarily used for reducing the number of features (dimensions) in a \n",
    "#dataset while preserving as much variance as possible. It transforms the original features into a new set of \n",
    "#orthogonal features (principal components) that capture the most important information in the data. The goal is \n",
    "#to achieve dimensionality reduction for computational efficiency, visualization, and dealing with the \"curse of \n",
    "#dimensionality.\"\n",
    "\n",
    "#PCA and Feature Extraction: Feature extraction involves transforming the original features into a new set of \n",
    "#features that represent the data in a more meaningful way. This process can involve creating new features based\n",
    "#on domain knowledge or extracting essential information from the original features. PCA can be used as a feature \n",
    "#extraction technique by selecting a subset of the principal components as the new features, effectively combining the \n",
    "#original features into a smaller number of meaningful features.\n",
    "\n",
    "#Example:\n",
    "\n",
    "#Suppose you have a dataset with three features: \"Temperature,\" \"Humidity,\" and \"Pressure.\" You want to extract\n",
    "#meaningful features while reducing dimensionality.\n",
    "\n",
    "#Original data:\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#data = np.array([\n",
    "    #[25, 60, 1012],\n",
    "    #[30, 50, 1008],\n",
    "    #[28, 55, 1010]\n",
    "#])\n",
    "\n",
    "#Using PCA for feature extraction:\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with 1 principal component\n",
    "#pca = PCA(n_components=1)\n",
    "\n",
    "# Fit and transform the data\n",
    "#data_transformed = pca.fit_transform(data)\n",
    "\n",
    "#print(\"Original data:\")\n",
    "#print(data)\n",
    "#print(\"\\nTransformed data:\")\n",
    "#print(data_transformed)\n",
    "\n",
    "#In this example, we are using PCA to extract one feature (principal component) from the original three features.\n",
    "#The \"Transformed data\" will contain the values projected onto this principal component. This process combines the \n",
    "#original features in a way that retains the most significant information in a lower-dimensional space.\n",
    "\n",
    "#Feature extraction using PCA can be useful when you have a high-dimensional dataset and want to create a reduced \n",
    "#set of features that capture the essential information. It can help improve model performance, decrease computation \n",
    "#time, and enhance visualization.\n",
    "\n",
    "#It's important to note that PCA might not always result in interpretable features in terms of their original meanings,\n",
    "#especially when used as feature extraction. In some cases, domain knowledge-driven feature extraction methods might be\n",
    "#more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01b1baf-b8a4-4785-85d2-3542cb2ca2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "#contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "#preprocess the data.\n",
    "#Answer.5 : In the context of building a recommendation system for a food delivery service, Min-Max scaling can be\n",
    "#a useful data preprocessing technique to bring the features within a consistent range, making them suitable for\n",
    "#modeling and analysis. Here's how you can use Min-Max scaling for the features like price, rating, and delivery time:\n",
    "\n",
    "#Understand the Features: First, you need to understand the range and distribution of each feature. For example,\n",
    "#the \"price\" could range from a few dollars to a higher value, \"rating\" could range from 0 to 5, and \"delivery time\" \n",
    "#could be expressed in minutes.\n",
    "\n",
    "#Apply Min-Max Scaling: Min-Max scaling scales the features to a specific range, often between 0 and 1. The formula \n",
    "#for Min-Max scaling is:\n",
    "\n",
    "    #Xscaled = (X-Xmin)/(Xmax - Xmin)\n",
    "\n",
    "#Where:\n",
    "\n",
    "#X is the original value of the feature.\n",
    "\n",
    "#Xmin is the minimum value of the feature in the dataset.\n",
    "\n",
    "#Xmax is the maximum value of the feature in the dataset.\n",
    "\n",
    "#X scaled is the scaled value of the feature between 0 and 1.\n",
    "#Implementation in Python: Here's how you might apply Min-Max scaling to your dataset using Python and the MinMaxScaler\n",
    "\n",
    "#from the sklearn.preprocessing module:\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "#data = pd.DataFrame({\n",
    "    #'price': [10, 20, 30, 40, 50],\n",
    "    #'rating': [3.5, 4.8, 3.2, 4.5, 2.7],\n",
    "    #'delivery_time': [25, 40, 30, 50, 20]\n",
    "#})\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "#scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert scaled data back to DataFrame for clarity\n",
    "#scaled_df = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "#print(\"Original data:\")\n",
    "#print(data)\n",
    "#print(\"\\nScaled data using Min-Max scaling:\")\n",
    "#print(scaled_df)\n",
    "#In this code, the MinMaxScaler scales each feature individually to the range [0, 1]. The scaled data is then converted\n",
    "#back into a DataFrame for better understanding.\n",
    "\n",
    "#By applying Min-Max scaling, you ensure that each feature's values are on a common scale, which can prevent features \n",
    "#with larger ranges from dominating the recommendation model's calculations. This enables your recommendation system to\n",
    "#consider each feature's contribution more fairly during the recommendation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eeaa1d5-c65a-45e5-bca7-4104bd96d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "#features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "#dimensionality of the dataset.\n",
    "#Answer.6 : When building a model to predict stock prices using a dataset with numerous features, employing PCA \n",
    "#(Principal Component Analysis) for dimensionality reduction can be highly beneficial. This technique helps in\n",
    "#managing high-dimensional data, improving model efficiency, and mitigating the effects of the \"curse of dimensionality.\" \n",
    "#Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "#Understand the Features: Begin by understanding the nature of the features in your dataset. You likely have \n",
    "#company-specific financial data (e.g., revenue, profit, debt) and market-related trends (e.g., interest rates,\n",
    "#GDP growth) among others. These features might be correlated, noisy, or redundant, which can hinder the performance \n",
    "#of your model.\n",
    "\n",
    "#Standardize the Data: To ensure that PCA works effectively, it's crucial to standardize the data by \n",
    "#subtracting the mean and dividing by the standard deviation. This step is essential because PCA is sensitive \n",
    "#to the scale of the features.\n",
    "\n",
    "#Calculate Covariance Matrix: Compute the covariance matrix of the standardized data. This matrix indicates how\n",
    "#features vary together, providing insights into their relationships.\n",
    "\n",
    "#Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "#Eigenvectors represent directions of maximum variance in the data, and eigenvalues denote the magnitude of\n",
    "#variance along those directions.\n",
    "\n",
    "#Select Principal Components: Sort the eigenvectors by their corresponding eigenvalues in descending order. \n",
    "#Choose the top k eigenvectors that contribute to most of the variance in the data. These are your principal components.\n",
    "\n",
    "#Project Data: Project the original data onto the selected principal components. This effectively reduces the \n",
    "#dimensionality of the dataset while retaining as much variance as possible.\n",
    "\n",
    "#Example:\n",
    "\n",
    "#Let's assume you have a dataset with 20 features related to company financial data and market trends. You want to use \n",
    "#PCA to reduce the dimensionality to 5 principal components.\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume data is loaded into a variable called 'data'\n",
    "\n",
    "# Standardize the data\n",
    "#scaler = StandardScaler()\n",
    "#data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Initialize PCA with 5 principal components\n",
    "#pca = PCA(n_components=5)\n",
    "\n",
    "# Apply PCA and transform the data\n",
    "#data_reduced = pca.fit_transform(data_scaled)\n",
    "\n",
    "#print(\"Original data shape:\", data.shape)\n",
    "#print(\"Reduced data shape:\", data_reduced.shape)\n",
    "\n",
    "#In this example, the standardized data is subjected to PCA, and the transformed data has been reduced to 5 principal\n",
    "#components. These components can be used as the new features for your stock price prediction model.\n",
    "\n",
    "#By performing PCA, you effectively reduce the number of features while retaining the most significant patterns and\n",
    "#variance in the data. This often leads to improved model performance, faster training times, and better generalization\n",
    "#to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8940bf1-3d0f-45c0-8322-62356fc4a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "#values to a range of -1 to 1.\n",
    "#Answer.7 : To perform Min-Max scaling and transform the values in the dataset [1,5,10,15,20] to a range of -1 to 1, \n",
    "#you can use the following formula:\n",
    "    \n",
    "#Xscaled = (((X-Xmin)/(Xmax-Xmin)) *2 -1\n",
    "           \n",
    "#Where:\n",
    "\n",
    "#X represents the original value in the dataset.\n",
    "\n",
    "#Xmin is the minimum value in the dataset.\n",
    "\n",
    "#Xmax is the maximum value in the dataset.\n",
    "\n",
    "#Xscaled is the scaled value in the range of -1 to 1.\n",
    "\n",
    "#Here's how you can apply this formula in Python to perform Min-Max scaling on the given dataset:\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "#data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate minimum and maximum values\n",
    "#min_val = data.min()\n",
    "#max_val = data.max()\n",
    "\n",
    "# Perform Min-Max scaling to range -1 to 1\n",
    "#scaled_data = ((data - min_val) / (max_val - min_val)) * 2 - 1\n",
    "\n",
    "#print(\"Original data:\", data)\n",
    "#print(\"Scaled data (-1 to 1):\", scaled_data)\n",
    "\n",
    "#Running this code will provide you with the scaled values of the dataset \n",
    "\n",
    "#[1,5,10,15,20] in the range of -1 to 1. The resulting scaled data will be:\n",
    "\n",
    "#Original data: [ 1  5 10 15 20]\n",
    "#Scaled data (-1 to 1): [-1.  -0.5  0.   0.5  1. ]\n",
    "#Now the values are transformed to the desired range of -1 to 1 using Min-Max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399c2b3-557e-444e-bdf1-6a37149c9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.8 : For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "#Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "#Answer.8 : To perform feature extraction using PCA on a dataset with features like [height, weight, age, gender,\n",
    "#blood pressure], you would follow these steps:\n",
    "\n",
    "#Preprocess the Data: Standardize the data by subtracting the mean and dividing by the standard deviation for\n",
    "#each feature. This step is crucial for PCA to work effectively since it is sensitive to the scale of the features.\n",
    "\n",
    "#Calculate Covariance Matrix: Compute the covariance matrix of the standardized data. The covariance matrix will \n",
    "#help you understand how features vary together and provide insights into their relationships.\n",
    "\n",
    "#Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix. \n",
    "#Eigenvectors represent directions of maximum variance, and eigenvalues indicate the magnitude of variance \n",
    "#along those directions.\n",
    "\n",
    "#Sort Eigenvectors by Eigenvalues: Arrange the eigenvectors in descending order based on their corresponding \n",
    "#eigenvalues. This step helps you identify the principal components that capture the most variance in the data.\n",
    "\n",
    "#Choose Principal Components: Select a subset of the eigenvectors (principal components) to retain for your\n",
    "#feature extraction. The number of components you choose determines the dimensionality of your new feature space.\n",
    "\n",
    "#Choosing the Number of Principal Components:\n",
    "\n",
    "#The decision on how many principal components to retain depends on the amount of variance you want to preserve. You \n",
    "#can evaluate the cumulative explained variance ratio to determine how many components are needed to capture a significant \n",
    "#portion of the variance in the original data. A common approach is to choose the number of components that collectively \n",
    "#explain a high percentage (e.g., 95% or more) of the total variance.\n",
    "\n",
    "#You can use the explained_variance_ratio_ attribute provided by PCA to see the proportion of variance explained by each \n",
    "#component and their cumulative explained variance.\n",
    "\n",
    "#Here's how you might use PCA and choose the number of principal components in Python:\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume 'data' is your dataset\n",
    "\n",
    "# Standardize the data\n",
    "#scaler = StandardScaler()\n",
    "#data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Initialize PCA\n",
    "#pca = PCA()\n",
    "\n",
    "# Fit PCA and calculate explained variance ratios\n",
    "#pca.fit(data_scaled)\n",
    "#explained_variances = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate cumulative explained variances\n",
    "#cumulative_variances = np.cumsum(explained_variances)\n",
    "\n",
    "# Find the number of components that explain a desired variance\n",
    "#desired_variance = 0.95\n",
    "#num_components = np.argmax(cumulative_variances >= desired_variance) + 1\n",
    "\n",
    "#print(\"Explained variances:\", explained_variances)\n",
    "#print(\"Cumulative variances:\", cumulative_variances)\n",
    "#print(\"Number of components to retain:\", num_components)\n",
    "#In this example, desired_variance is set to 0.95 to retain at least 95% of the variance. The num_components variable\n",
    "#indicates the number of principal components you would retain to achieve this level of variance preservation.\n",
    "\n",
    "#Remember that the choice of the number of principal components can also depend on your specific application,\n",
    "#computational resources, and the trade-off between dimensionality reduction and information preservation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
