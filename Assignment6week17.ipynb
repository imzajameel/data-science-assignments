{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd41810-d198-4e96-99b2-eab14fcfa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.17 \n",
    "#Assignment.6 \n",
    "#Question.1 : You are working on a machine learning project where you have a dataset containing numerical and\n",
    "#categorcal features. You have identified that some of the features are highly correlated and there are\n",
    "#missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "#engneerng process and handles the missing valuesD\n",
    "#Design a pipeline that includes the following steps\"\n",
    "#Use an automated feature select#on method to identify the important features in the dataset\n",
    "#Create a numerical pipeline that #ncludes the following steps\"\n",
    "#Impute the missing values in the numerical columns using the mean of the column values\n",
    "#Scale the numerical columns using standardisation\n",
    "#Create a categorical pipeline that includes the follow#ng steps\"\n",
    "#Impute the missing values in the categorical columns us#ng the most frequent value of the column\n",
    "#One-hot encode the categorical columns\n",
    "#Combine the numerical and categorical pipelines using a ColumnTransformer\n",
    "#Use a Random Forest Classifier to build the final model\n",
    "#Evaluate the accuracy of the model on the test datasetD\n",
    "#Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "#each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "#the pipeline\n",
    "#Answer.1 : \n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Assume 'X_train', 'X_test', 'y_train', 'y_test' are your training and testing data\n",
    "\n",
    "# Step 1: Feature Selection\n",
    "#feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Step 2: Numerical Pipeline\n",
    "#numerical_pipeline = Pipeline([\n",
    " #   ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "#    ('scaler', StandardScaler())  # Standardize numerical columns\n",
    "#])\n",
    "\n",
    "# Step 3: Categorical Pipeline\n",
    "#categorical_pipeline = Pipeline([\n",
    "#    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value\n",
    "#    ('encoder', OneHotEncoder())  # One-hot encode categorical columns\n",
    "#])\n",
    "\n",
    "# Step 4: Combine Numerical and Categorical Pipelines\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numerical_pipeline, numerical_features),  # Specify numerical features\n",
    "#        ('cat', categorical_pipeline, categorical_features)  # Specify categorical features\n",
    "#    ])\n",
    "\n",
    "# Step 5: Final Pipeline with Feature Selection and Classifier\n",
    "#pipeline = Pipeline([\n",
    "#    ('feature_selector', feature_selector),\n",
    "#    ('preprocessor', preprocessor),\n",
    "#    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "#])\n",
    "\n",
    "# Step 6: Fit and Evaluate the Model\n",
    "#pipeline.fit(X_train, y_train)\n",
    "#accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "#print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Additional Steps:\n",
    "# - Interpretation of Results\n",
    "#   - Analyze feature importances obtained from the RandomForestClassifier to interpret the impact of\n",
    "#features on the model.\n",
    "#   - Evaluate model performance metrics beyond accuracy, such as precision, recall, and F1-score.\n",
    "\n",
    "# - Possible Improvements\n",
    "#   - Fine-tune hyperparameters of the Random Forest Classifier for better model performance.\n",
    "#   - Experiment with different feature selection techniques and imputation strategies.\n",
    "#   - Explore other algorithms or ensemble methods for potential performance gains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd6a112-e0a5-4e4b-8425-0a74989f042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : Build a pipeline that includes a random forest classifier and a logistic regression classsfier, and then\n",
    "#use a voting classifier\n",
    "#Answer.2 :\n",
    "#from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume 'X' and 'y' are your features and target variable\n",
    "#X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Create individual classifiers\n",
    "#rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Step 2: Create a pipeline for each classifier (optional preprocessing steps can be added)\n",
    "#rf_pipeline = Pipeline([\n",
    "#    ('scaler', StandardScaler()),  # Optional: Standardize features\n",
    "#    ('rf', rf_classifier)\n",
    "#])\n",
    "\n",
    "#lr_pipeline = Pipeline([\n",
    "#    ('scaler', StandardScaler()),  # Optional: Standardize features\n",
    "#    ('lr', lr_classifier)\n",
    "#])\n",
    "\n",
    "# Step 3: Create a Voting Classifier\n",
    "#voting_classifier = VotingClassifier(\n",
    " #   estimators=[\n",
    " #       ('random_forest', rf_pipeline),\n",
    " #       ('logistic_regression', lr_pipeline)\n",
    " #   ],\n",
    " #   voting='hard'  # 'hard' for majority voting, 'soft' for weighted voting based on predicted probabilities\n",
    "#)\n",
    "\n",
    "# Step 4: Fit and Evaluate the Model\n",
    "#voting_classifier.fit(X_train, y_train)\n",
    "#accuracy = voting_classifier.score(X_test, y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "#print(f\"Voting Classifier Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
