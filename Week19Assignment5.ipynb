{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba880f9-19fa-42ad-80af-58a3e1da5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "#Week.19 \n",
    "#Assignment.5 \n",
    "#Question.1 : What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "#Answer.1 : # Contingency Matrix in Classification Evaluation:\n",
    "\n",
    "# A contingency matrix, also known as a confusion matrix, is used to evaluate the performance of a classification model.\n",
    "# It summarizes the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Example true labels and predicted labels\n",
    "true_labels = [1, 0, 1, 1, 0, 1, 0, 0]\n",
    "predicted_labels = [1, 0, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Output:\n",
    "# [[2 2]\n",
    "#  [2 2]]\n",
    "\n",
    "# Interpretation of the confusion matrix:\n",
    "# - The rows represent the actual (true) classes, and the columns represent the predicted classes.\n",
    "# - The top-left (2) and bottom-right (2) elements are the counts of true positives (TP).\n",
    "# - The top-right (2) element is the count of false positives (FP).\n",
    "# - The bottom-left (2) element is the count of false negatives (FN).\n",
    "\n",
    "# Contingency matrix provides valuable information for various classification metrics, including accuracy, precision,\n",
    "#recall, and F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44a8eac-b63e-4cc9-82ab-2b7bb73fcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Confusion Matrix:\n",
      "[[2 2]\n",
      " [1 3]]\n",
      "\n",
      "Pair Confusion Matrix:\n",
      "[[2 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "#Question.2 : How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "#certain situations?\n",
    "#Answer.2 : # Pair Confusion Matrix:\n",
    "\n",
    "# A pair confusion matrix is a specialized form of confusion matrix used in binary classification problems\n",
    "# when the goal is to distinguish between two specific classes, typically positive and negative.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Example true labels and predicted labels\n",
    "true_labels = [1, 0, 1, 1, 0, 1, 0, 0]\n",
    "predicted_labels = [1, 0, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "# Calculate the regular confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate the pair confusion matrix for positive class (class 1)\n",
    "pair_conf_matrix = conf_matrix[:2, :2]\n",
    "\n",
    "print(\"Regular Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nPair Confusion Matrix:\")\n",
    "print(pair_conf_matrix)\n",
    "\n",
    "# Output:\n",
    "# Regular Confusion Matrix:\n",
    "# [[2 2]\n",
    "#  [2 2]]\n",
    "#\n",
    "# Pair Confusion Matrix:\n",
    "# [[2 2]\n",
    "#  [2 2]]\n",
    "\n",
    "# In this example, the regular confusion matrix and pair confusion matrix are identical as there are only two classes.\n",
    "\n",
    "# Pair confusion matrix is useful when the focus is specifically on two classes, and it simplifies the analysis by\n",
    "# considering only the relevant entries for those classes (true positives, false positives, false negatives, and true \n",
    "#negatives).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a7501a-c773-4b8f-9abc-1dbb649e692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "#used to evaluate the performance of language models?\n",
    "#Answer.3 : # Extrinsic Measure in Natural Language Processing (NLP):\n",
    "\n",
    "# An extrinsic measure in NLP refers to the evaluation of a language model's performance in the context of a specific\n",
    "# downstream task or application. Unlike intrinsic measures that assess a model's capabilities in isolation, extrinsic\n",
    "# measures involve evaluating the model's contribution to solving real-world problems.\n",
    "\n",
    "# Example:\n",
    "# Consider a language model trained for sentiment analysis. The extrinsic measure would involve assessing the\n",
    "# model's performance on sentiment classification tasks using metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "# Example Evaluation Code (using scikit-learn):\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are the training and test sets\n",
    "#model = LogisticRegression()  # Replace with the actual language model\n",
    "\n",
    "# Train the model on the training set\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "#predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate using extrinsic measures\n",
    "#accuracy = accuracy_score(y_test, predictions)\n",
    "#precision = precision_score(y_test, predictions)\n",
    "#recall = recall_score(y_test, predictions)\n",
    "#f1 = f1_score(y_test, predictions)\n",
    "\n",
    "#print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#print(f\"Precision: {precision:.4f}\")\n",
    "#print(f\"Recall: {recall:.4f}\")\n",
    "#print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# In this example, accuracy, precision, recall, and F1 score are extrinsic measures used to evaluate the model's\n",
    "# performance in the context of sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b646b404-bb30-4680-8c1f-cb14cbf9d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "#extrinsic measure?\n",
    "#Answer.4 : # Intrinsic Measure in Machine Learning:\n",
    "\n",
    "# An intrinsic measure in machine learning refers to the evaluation of a model's performance based on its internal\n",
    "# characteristics and capabilities, without considering its application in a specific task or real-world scenario.\n",
    "# These measures assess the model's inherent qualities, such as its ability to learn patterns, generalize to new data,\n",
    "# and adapt to different complexities.\n",
    "\n",
    "# Example:\n",
    "# A common intrinsic measure is the mean squared error (MSE) for regression tasks. MSE evaluates how well a regression\n",
    "# model captures the variance in the data, without considering the model's application in a particular prediction task.\n",
    "\n",
    "# Example Evaluation Code (using scikit-learn):\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are the training and test sets\n",
    "#model = LinearRegression()  # Replace with the actual machine learning model\n",
    "\n",
    "# Train the model on the training set\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "#predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate using intrinsic measure (Mean Squared Error)\n",
    "#mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "#print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# In this example, Mean Squared Error is an intrinsic measure that quantifies the model's prediction accuracy,\n",
    "# irrespective of the specific application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84712924-b843-463e-9849-004411bca21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "#strengths and weaknesses of a model?\n",
    "#Answer.5 : # Confusion Matrix in Machine Learning:\n",
    "\n",
    "# A confusion matrix is a table that is used to evaluate the performance of a classification model by\n",
    "# presenting a clear and detailed summary of the model's predictions versus the actual outcomes in a\n",
    "# tabular form. It is particularly useful for identifying strengths and weaknesses of a model.\n",
    "\n",
    "# Example:\n",
    "# Let's assume a binary classification problem with classes \"Positive\" and \"Negative.\"\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are the training and test sets\n",
    "#model = LogisticRegression()  # Replace with the actual machine learning model\n",
    "\n",
    "# Train the model on the training set\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "#predictions = model.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "#cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(cm)\n",
    "\n",
    "# The resulting confusion matrix looks like this:\n",
    "# [[True Negative, False Positive],\n",
    "#  [False Negative, True Positive]]\n",
    "\n",
    "# Interpretation:\n",
    "# - True Positive (TP): Correctly predicted positive instances\n",
    "# - True Negative (TN): Correctly predicted negative instances\n",
    "# - False Positive (FP): Incorrectly predicted positive instances (Type I error)\n",
    "# - False Negative (FN): Incorrectly predicted negative instances (Type II error)\n",
    "\n",
    "# Strengths and Weaknesses:\n",
    "# - Strengths: Confusion matrices provide a detailed breakdown of the model's performance, allowing for\n",
    "#   a clear understanding of its accuracy and error types.\n",
    "# - Weaknesses: While informative, confusion matrices do not provide a single metric for model evaluation,\n",
    "#   and interpretation may require domain knowledge.\n",
    "\n",
    "# Example Metrics from Confusion Matrix:\n",
    "# - Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "# - Precision: TP / (TP + FP)\n",
    "# - Recall (Sensitivity): TP / (TP + FN)\n",
    "# - F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "# The confusion matrix is a powerful tool for model evaluation, especially in binary and multiclass classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51de2022-79b1-40d6-b236-1d13c959b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "#learning algorithms, and how can they be interpreted?\n",
    "#Answer.6 : # Intrinsic Measures for Evaluating Unsupervised Learning Algorithms:\n",
    "\n",
    "# Unsupervised learning algorithms, particularly clustering algorithms, are often evaluated using intrinsic measures\n",
    "# that assess the quality of the clusters formed without relying on external labels. Common intrinsic measures include\n",
    "# Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index.\n",
    "\n",
    "#from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming X is the feature matrix\n",
    "# Example clustering algorithm: KMeans\n",
    "#kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "#labels = kmeans.fit_predict(X)\n",
    "\n",
    "# 1. Silhouette Score:\n",
    "#silhouette_avg = silhouette_score(X, labels)\n",
    "#print(f'Silhouette Score: {silhouette_avg}')\n",
    "# - Interpreted as the average silhouette coefficient across all data points.\n",
    "# - Ranges from -1 to 1, where a higher score indicates better-defined clusters.\n",
    "# - A positive value suggests that the object is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "# 2. Davies-Bouldin Index:\n",
    "#db_index = davies_bouldin_score(X, labels)\n",
    "#print(f'Davies-Bouldin Index: {db_index}')\n",
    "# - Measures the compactness and separation between clusters.\n",
    "# - Lower values indicate better clustering, with 0 being the ideal value.\n",
    "# - Useful for comparing different clusterings.\n",
    "\n",
    "# 3. Calinski-Harabasz Index:\n",
    "#ch_index = calinski_harabasz_score(X, labels)\n",
    "#print(f'Calinski-Harabasz Index: {ch_index}')\n",
    "# - Also known as the Variance Ratio Criterion.\n",
    "# - Measures the ratio of between-cluster variance to within-cluster variance.\n",
    "# - Higher values indicate better-defined clusters.\n",
    "\n",
    "# Note: Interpretation of these scores depends on the specific algorithm and dataset characteristics. It is essential\n",
    "# to consider multiple metrics and domain knowledge for a comprehensive evaluation of clustering quality.\n",
    "\n",
    "# These intrinsic measures are valuable for assessing the internal coherence and separation of clusters in unsupervised\n",
    "#learning,\n",
    "# providing insights into the effectiveness of clustering algorithms without the need for ground truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96f4ea-8c5b-425e-a275-eb349767a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "#how can these limitations be addressed?\n",
    "#Answer.7 : \n",
    "# Limitations of Using Accuracy as a Sole Evaluation Metric for Classification Tasks:\n",
    "\n",
    "# Accuracy alone may not provide a complete picture of a classifier's performance, and it has certain limitations.\n",
    "# Considerations such as class imbalance, misclassification costs, and the nature of the problem can impact the\n",
    "# usefulness of accuracy. Here are some limitations and ways to address them:\n",
    "\n",
    "# 1. Class Imbalance:\n",
    "#    - In imbalanced datasets, where one class significantly outnumbers others, accuracy can be misleading.\n",
    "#    - Address by using alternative metrics such as precision, recall, F1-score, or area under the ROC curve (AUC-ROC).\n",
    "\n",
    "# 2. Misclassification Costs:\n",
    "#    - Different types of errors (false positives and false negatives) may have varying consequences.\n",
    "#    - Utilize metrics like precision and recall, which focus on specific aspects of misclassification, based on the\n",
    "#problem requirements.\n",
    "\n",
    "# 3. Nature of the Problem:\n",
    "#    - For certain applications, the cost of false positives and false negatives may differ.\n",
    "#    - Tailor the evaluation metrics to the specific needs of the problem, emphasizing the aspects critical to the task.\n",
    "\n",
    "# Example using precision, recall, and F1-score:\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1-Score: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
