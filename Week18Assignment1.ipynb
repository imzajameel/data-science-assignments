{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becee156-3ace-4731-9690-d2e3f0ef4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.18 \n",
    "#Assignment.1 \n",
    "#Question.1 : What is the KNN algorithm?\n",
    "#Answer.1 : # KNN, or k-Nearest Neighbors, is a machine learning algorithm for classification and regression tasks.\n",
    "\n",
    "# 1. Initialization: Choose the number of neighbors (k) and a distance metric.\n",
    "\n",
    "# 2. Training: During training, the algorithm stores the entire training dataset.\n",
    "\n",
    "# 3. Prediction (Classification): To predict the class of a new data point, calculate distances\n",
    "#    between the new point and all points in the training set. Select k-nearest neighbors based on distances.\n",
    "#    For classification, assign the most common class label among the k-nearest neighbors to the new data point.\n",
    "\n",
    "# 4. Prediction (Regression): For regression, predict the average or weighted average of the target variable\n",
    "#    for the k-nearest neighbors instead of assigning a class label.\n",
    "\n",
    "# 5. Evaluation: Evaluate the algorithm's performance using metrics depending on the task (classification or regression).\n",
    "\n",
    "# Note: The choice of distance metric (e.g., Euclidean, Manhattan) and the value of k can significantly impact\n",
    "# the algorithm's performance. KNN is simple but can be computationally expensive with large datasets.\n",
    "# Careful consideration of k and the distance metric is important based on the data characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553e24f7-1bdd-4eaf-96f0-00c454010cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : How do you choose the value of K in KNN?\n",
    "#Answer.2 : # Choosing the value of k in KNN is a critical step that impacts model performance.\n",
    "\n",
    "# 1. Odd vs. Even: For binary classification, choose an odd k to prevent ties when determining the majority class.\n",
    "\n",
    "# 2. Domain Knowledge: Consider the nature of your dataset and problem domain. Certain k values may align better\n",
    "#with the data.\n",
    "\n",
    "# 3. Square Root Rule: Set k to the square root of the number of data points for a balance between overfitting and\n",
    "#underfitting.\n",
    "#    import math\n",
    "#    k = int(math.sqrt(len(training_data)))\n",
    "\n",
    "# 4. Cross-Validation: Use techniques like k-fold cross-validation to assess performance for different k values and \n",
    "#choose one that generalizes well.\n",
    "\n",
    "# 5. Experimentation: Try different k values and observe their impact on model performance. Plot performance metrics\n",
    "#against k values for insights.\n",
    "\n",
    "# 6. Grid Search: If resources allow, perform a grid search over a range of k values to find the optimal one that\n",
    "#maximizes performance.\n",
    "\n",
    "# 7. Consider Data Size: For smaller datasets, use a smaller k to prevent overfitting. For larger datasets, a slightly\n",
    "#larger k might be suitable.\n",
    "\n",
    "# 8. Weighted KNN: In some cases, use weighted KNN with inverse distance weights to give more influence to closer neighbors.\n",
    "\n",
    "# Remember, there is no one-size-fits-all solution. Experiment and validate the model's performance with different k \n",
    "#values and techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a65582-2809-4d9c-bc76-a30f6e713311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : What is the difference between KNN classifier and KNN regressor?\n",
    "#Answer.3 : # KNN Classifier:\n",
    "# - Task: Used for classification problems where the goal is to assign a class label to a given input.\n",
    "# - Output: Predicts the class membership based on the majority class among its k-nearest neighbors.\n",
    "# - Example: Predicting the type of fruit (e.g., apples, oranges, bananas) based on features like color and size.\n",
    "\n",
    "# Example code for KNN Classifier:\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn_classifier.fit(X_train, y_train)\n",
    "# predicted_class = knn_classifier.predict(new_data_point)\n",
    "\n",
    "# KNN Regressor:\n",
    "# - Task: Used for regression problems where the goal is to predict a continuous numerical value for a given input.\n",
    "# - Output: Predicts the average or weighted average of the target variable among its k-nearest neighbors.\n",
    "# - Example: Predicting the price of a house based on features like square footage and number of bedrooms.\n",
    "\n",
    "# Example code for KNN Regressor:\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "# knn_regressor.fit(X_train, y_train)\n",
    "# predicted_value = knn_regressor.predict(new_data_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98376731-13e7-4e9b-ad73-22a5022f12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : How do you measure the performance of KNN?\n",
    "#Answer.4 : # For Classification Tasks:\n",
    "\n",
    "# Accuracy:\n",
    "# - Measures the overall correctness of the classifier.\n",
    "# - Formula: (Number of Correct Predictions) / (Total Number of Predictions)\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Precision, Recall, and F1-Score:\n",
    "# - Precision: (True Positives) / (True Positives + False Positives)\n",
    "# - Recall: (True Positives) / (True Positives + False Negatives)\n",
    "# - F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# - Useful when dealing with imbalanced classes.\n",
    "#from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "#precision = precision_score(y_true, y_pred)\n",
    "#recall = recall_score(y_true, y_pred)\n",
    "#f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Confusion Matrix:\n",
    "# - Provides a detailed breakdown of correct and incorrect predictions.\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC-ROC):\n",
    "# - Particularly useful for binary classification.\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# For Regression Tasks:\n",
    "\n",
    "# Mean Absolute Error (MAE):\n",
    "# - Represents the average absolute difference between the predicted and actual values.\n",
    "# - Formula: mean(|actual - predicted|)\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "#mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):\n",
    "# - MSE Formula: mean((actual - predicted)^2)\n",
    "# - RMSE Formula: sqrt(MSE)\n",
    "# - RMSE is more sensitive to large errors.\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#mse = mean_squared_error(y_true, y_pred)\n",
    "#rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "# R^2 Score (Coefficient of Determination):\n",
    "# - Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "#from sklearn.metrics import r2_score\n",
    "#r2 = r2_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f868c3b0-72df-4ff3-99ec-6bde3c1cfa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : What is the curse of dimensionality in KNN?\n",
    "#Answer.5 : # The curse of dimensionality refers to challenges in high-dimensional data affecting algorithms like KNN.\n",
    "\n",
    "# 1. Increased Computational Complexity:\n",
    "#    - Distance calculations become computationally expensive as dimensions increase.\n",
    "#    - The volume of the feature space grows exponentially.\n",
    "\n",
    "# 2. Diminishing Data Density:\n",
    "#    - High-dimensional spaces lead to sparser data.\n",
    "#    - Proximity becomes less meaningful, making it challenging to identify meaningful neighbors.\n",
    "\n",
    "# 3. Increased Sensitivity to Noise:\n",
    "#    - High dimensions increase the chance of irrelevant features introducing noise.\n",
    "#    - Noise can impact accuracy by giving undue importance to less informative features.\n",
    "\n",
    "# 4. Loss of Discriminatory Information:\n",
    "#    - Redundancy and correlated features may occur.\n",
    "#    - KNN may treat features as equally important, leading to a loss of discriminatory information.\n",
    "\n",
    "# 5. Overfitting and Poor Generalization:\n",
    "#    - Limited representative data in high-dimensional spaces can lead to overfitting.\n",
    "#    - Poor generalization to new, unseen data may occur.\n",
    "\n",
    "# 6. Difficulty in Choosing an Optimal k:\n",
    "#    - Choosing the number of neighbors (k) becomes challenging in high-dimensional spaces.\n",
    "#    - Small k may result in overfitting, while large k may increase computational costs and lose local patterns.\n",
    "\n",
    "# Mitigation Strategies:\n",
    "# - Consider dimensionality reduction techniques (e.g., PCA) to reduce features.\n",
    "# - Use feature selection methods to focus on relevant features.\n",
    "# - Explore alternative algorithms less sensitive to high dimensionality for specific scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d554353-0c04-4569-a437-54ce36d675b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : How do you handle missing values in KNN?\n",
    "#Answer.6 : # Handling missing values in KNN involves imputing or filling in the missing values for effective predictions.\n",
    "\n",
    "# 1. Imputation with Mean, Median, or Mode:\n",
    "#    - Substitute missing values with the mean, median, or mode of non-missing values for the respective feature.\n",
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer with the desired strategy (mean, median, mode)\n",
    "#imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the data with imputer\n",
    "#X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "\n",
    "# 2. KNN Imputation:\n",
    "#    - Use the k-Nearest Neighbors algorithm to impute missing values.\n",
    "#    - Estimates missing values based on the values of their k-nearest neighbors.\n",
    "#from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a KNN imputer with the desired number of neighbors\n",
    "#knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit and transform the data with KNN imputer\n",
    "#X_imputed = knn_imputer.fit_transform(X)\n",
    "\n",
    "\n",
    "# 3. Imputation with Regression Models:\n",
    "#    - Train regression models (e.g., linear regression) to predict missing values based on other features.\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Identify features with missing values\n",
    "#missing_features = ['feature1', 'feature2']\n",
    "\n",
    "#for feature in missing_features:\n",
    "    # Create a regression model\n",
    "#    regression_model = LinearRegression()\n",
    "\n",
    "    # Separate data into non-missing and missing values\n",
    "#   non_missing = X[X[feature].notnull()]\n",
    "#   missing = X[X[feature].isnull()]\n",
    "\n",
    "    # Fit regression model\n",
    " #  regression_model.fit(non_missing.drop(feature, axis=1), non_missing[feature])\n",
    "\n",
    "    # Predict missing values\n",
    "#   X.loc[X[feature].isnull(), feature] = regression_model.predict(missing.drop(feature, axis=1))\n",
    "\n",
    "\n",
    "# 4. Use of Advanced Imputation Methods:\n",
    "#    - Explore advanced imputation methods such as MICE (Multiple Imputation by Chained Equations) or other ML-based\n",
    "#techniques.\n",
    "#rom sklearn.impute import IterativeImputer\n",
    "\n",
    "# Create an iterative imputer\n",
    "#terative_imputer = IterativeImputer()\n",
    "\n",
    "# Fit and transform the data with iterative imputer\n",
    "#_imputed = iterative_imputer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d50edcd4-1fbe-46c2-b7db-434089cbcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "#which type of problem?\n",
    "#Answer.7 : # KNN Classifier:\n",
    "\n",
    "# Purpose:\n",
    "# - Designed for classification tasks where the goal is to assign a class label to each data point.\n",
    "\n",
    "# Output:\n",
    "# - Outputs the class label of a data point based on the majority class among its k-nearest neighbors.\n",
    "\n",
    "# Performance Metrics:\n",
    "# - Accuracy, precision, recall, F1-score, confusion matrix, ROC curve, AUC-ROC, etc.\n",
    "\n",
    "# Use Cases:\n",
    "# - Suitable for problems where the target variable is categorical or discrete (e.g., spam or not spam, image\n",
    "#recognition, sentiment analysis).\n",
    "# - Appropriate when dealing with classification tasks with two or more classes.\n",
    "\n",
    "# Considerations:\n",
    "# - Choose odd values of k to avoid ties in binary classification.\n",
    "# - Sensitive to outliers and imbalances in class distribution.\n",
    "\n",
    "\n",
    "# KNN Regressor:\n",
    "\n",
    "# Purpose:\n",
    "# - Designed for regression tasks where the goal is to predict a continuous numerical value for each data point.\n",
    "\n",
    "# Output:\n",
    "# - Outputs the average or weighted average of the target variable among its k-nearest neighbors.\n",
    "\n",
    "# Performance Metrics:\n",
    "# - Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R^2 score, etc.\n",
    "\n",
    "# Use Cases:\n",
    "# - Suitable for problems where the target variable is continuous (e.g., predicting house prices, stock prices, temperature).\n",
    "# - Appropriate when dealing with regression tasks where the output is a numerical value.\n",
    "\n",
    "# Considerations:\n",
    "# - Choose an appropriate value for k based on the characteristics of the data.\n",
    "# - Can be sensitive to outliers in the target variable.\n",
    "\n",
    "\n",
    "# Choosing Between KNN Classifier and Regressor:\n",
    "\n",
    "# Nature of the Target Variable:\n",
    "# - Choose KNN classifier for classification problems with categorical target variables.\n",
    "# - Choose KNN regressor for regression problems with continuous target variables.\n",
    "\n",
    "# Problem Requirements:\n",
    "# - Consider the specific requirements of the problem. Does it involve predicting classes or predicting numerical values?\n",
    "\n",
    "# Data Characteristics:\n",
    "# - Consider the characteristics of the dataset, such as the distribution of the target variable, the presence of outliers,\n",
    "#and the nature of the features.\n",
    "\n",
    "# Performance Metrics:\n",
    "# - Choose the model that aligns with the evaluation metrics relevant to your problem (e.g., accuracy for classification\n",
    "#, MAE or RMSE for regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f80bd74-1e45-4c04-b99a-c9c3d36fc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.8 : What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "#and how can these be addressed?\n",
    "#Answer.8 : # Strengths of KNN:\n",
    "\n",
    "# 1. Simple and Intuitive:\n",
    "#    - Strength: KNN is easy to understand and implement, making it a good choice for quick prototyping and baseline models.\n",
    "\n",
    "# 2. Non-parametric:\n",
    "#    - Strength: KNN is non-parametric, meaning it doesn't make assumptions about the underlying data distribution. \n",
    "#It can adapt to complex patterns.\n",
    "\n",
    "# 3. No Training Phase:\n",
    "#    - Strength: KNN doesn't have a separate training phase; the model is built at the time of prediction. This \n",
    "#can be advantageous for dynamic datasets.\n",
    "\n",
    "# 4. Effective for Locally Smooth Decision Boundaries:\n",
    "#    - Strength: KNN performs well when decision boundaries are not globally complex but vary locally.\n",
    "\n",
    "\n",
    "# Weaknesses of KNN:\n",
    "\n",
    "# 1. Computational Complexity:\n",
    "#    - Weakness: Calculating distances between data points becomes computationally expensive as the dataset size \n",
    "#increases, especially in high-dimensional spaces.\n",
    "\n",
    "# 2. Sensitivity to Irrelevant Features:\n",
    "#    - Weakness: KNN can be sensitive to irrelevant or redundant features, as all features contribute equally to the\n",
    "#distance computation.\n",
    "\n",
    "# 3. Impact of Outliers:\n",
    "#    - Weakness: Outliers can significantly affect the predictions, as they may become influential neighbors due \n",
    "#to their proximity.\n",
    "\n",
    "# 4. Need for Optimal K:\n",
    "#    - Weakness: The choice of the number of neighbors (k) is crucial and can impact model performance. A suboptimal\n",
    "#choice may lead to overfitting or underfitting.\n",
    "\n",
    "\n",
    "# Addressing Weaknesses:\n",
    "\n",
    "# 1. Computational Complexity:\n",
    "#    - Addressing: Use dimensionality reduction techniques (e.g., PCA) to reduce the number of features, or consider \n",
    "#algorithms that are less sensitive to high dimensionality.\n",
    "\n",
    "# 2. Sensitivity to Irrelevant Features:\n",
    "#    - Addressing: Perform feature selection to focus on the most relevant features. Experiment with different\n",
    "#distance metrics that give appropriate weight to relevant features.\n",
    "\n",
    "# 3. Impact of Outliers:\n",
    "#    - Addressing: Preprocess data to identify and handle outliers. Consider using weighted KNN or robust distance \n",
    "#metrics to reduce the influence of outliers.\n",
    "\n",
    "# 4. Need for Optimal K:\n",
    "#    - Addressing: Use cross-validation techniques to find the optimal k for your specific dataset. Experiment with\n",
    "#different k values and observe their impact on model performance.\n",
    "\n",
    "# 5. Distance Metric Selection:\n",
    "#    - Addressing: Experiment with different distance metrics (e.g., Euclidean, Manhattan, Minkowski) to find the one \n",
    "#that suits your data distribution. Customized distance metrics can also be defined based on domain knowledge.\n",
    "\n",
    "# 6. Ensemble Methods:\n",
    "#    - Addressing: Consider using ensemble methods like bagging or boosting with KNN to improve robustness and generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e63826-2c58-4bdf-8267-9f84157cc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.9 : What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "#Answer.9 : # Euclidean Distance:\n",
    "\n",
    "# Formula:\n",
    "# Euclidean Distance (A, B) = sqrt(sum((A_i - B_i)^2))\n",
    "\n",
    "# Characteristics:\n",
    "# - Also known as L2 norm or straight-line distance.\n",
    "# - Measures the straight-line distance between two points in Euclidean space.\n",
    "# - Reflects the actual distance between points, considering both horizontal and vertical movements.\n",
    "\n",
    "# Usage:\n",
    "# - Commonly used when the relationship between features is expected to be isotropic (uniform in all directions).\n",
    "# - Sensitive to magnitudes of individual features.\n",
    "\n",
    "\n",
    "# Manhattan Distance:\n",
    "\n",
    "# Formula:\n",
    "# Manhattan Distance (A, B) = sum(|A_i - B_i|)\n",
    "\n",
    "# Characteristics:\n",
    "# - Also known as L1 norm or city block distance.\n",
    "# - Measures the distance between two points by moving only horizontally and vertically (no diagonals).\n",
    "# - Represents the distance traveled along the grid lines of a city block.\n",
    "\n",
    "# Usage:\n",
    "# - Suitable when movement can only occur along axes (e.g., in urban navigation where movement is restricted to streets).\n",
    "# - Less sensitive to outliers compared to Euclidean distance.\n",
    "\n",
    "\n",
    "# Differences:\n",
    "\n",
    "# 1. Direction of Measurement:\n",
    "#    - Euclidean Distance: Measures the straight-line distance, considering both horizontal and vertical movements.\n",
    "#    - Manhattan Distance: Measures the distance by moving only horizontally and vertically along grid lines.\n",
    "\n",
    "# 2. Sensitivity to Magnitudes:\n",
    "#    - Euclidean Distance: Sensitive to magnitudes of individual features due to the squared term in the formula.\n",
    "#    - Manhattan Distance: Treats magnitudes of features equally since it only considers absolute differences.\n",
    "\n",
    "# 3. Isotropy vs. Anisotropy:\n",
    "#    - Euclidean Distance: Assumes isotropic (uniform) relationships between features.\n",
    "#    - Manhattan Distance: Useful when the relationships between features are anisotropic (vary in different directions).\n",
    "\n",
    "# 4. Geometric Interpretation:\n",
    "#    - Euclidean Distance: Represents the length of the shortest path between two points.\n",
    "#    - Manhattan Distance: Represents the sum of the lengths along each axis.\n",
    "\n",
    "# In the context of KNN, the choice between Euclidean and Manhattan distance depends on the characteristics of the data\n",
    "#and the nature of the relationships between features.\n",
    "# Experimenting with both distance metrics and observing their impact on model performance can guide the selection of an\n",
    "#appropriate metric for a specific problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084898bb-e41a-436d-b286-8789718d26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.10 : What is the role of feature scaling in KNN?\n",
    "#Answer.10 : # Role of Feature Scaling in KNN:\n",
    "\n",
    "# 1. Distance Metric Sensitivity:\n",
    "#    - KNN relies on distance metrics (e.g., Euclidean, Manhattan) for nearest neighbor identification.\n",
    "#    - Features with different scales can disproportionately affect the contribution of each feature to distance calculations.\n",
    "\n",
    "# 2. Equalizing Feature Influence:\n",
    "#    - Feature scaling ensures equal influence of all features.\n",
    "#    - Without scaling, features with larger magnitudes might dominate distance computations, leading to biased results.\n",
    "\n",
    "# 3. Magnitude Independence:\n",
    "#    - Feature scaling makes the algorithm independent of individual feature magnitudes.\n",
    "#    - KNN should focus on patterns and relationships between features, regardless of their original scales.\n",
    "\n",
    "# 4. Improved Model Performance:\n",
    "#    - Scaling features often results in improved model performance.\n",
    "#    - It aids faster convergence during optimization and prevents features from having undue impact on model decisions.\n",
    "\n",
    "# Common Feature Scaling Techniques:\n",
    "\n",
    "# 1. Min-Max Scaling (Normalization):\n",
    "#    - Formula: X_normalized = (X - min(X)) / (max(X) - min(X))\n",
    "#    - Scales features to a specific range (commonly [0, 1]).\n",
    "\n",
    "# 2. Standardization (Z-score normalization):\n",
    "#    - Formula: X_standardized = (X - mean(X)) / std(X)\n",
    "#    - Scales features to have zero mean and unit variance.\n",
    "\n",
    "# 3. Robust Scaling:\n",
    "#    - Formula: X_robust = (X - median(X)) / IQR(X)\n",
    "#    - Scales features based on median and interquartile range, robust to outliers.\n",
    "\n",
    "# Implementing Feature Scaling in Python:\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Example data\n",
    "#X = ...\n",
    "\n",
    "# Min-Max Scaling (Normalization)\n",
    "#scaler_minmax = MinMaxScaler()\n",
    "#X_minmax = scaler_minmax.fit_transform(X)\n",
    "\n",
    "# Standardization\n",
    "#scaler_standard = StandardScaler()\n",
    "#X_standard = scaler_standard.fit_transform(X)\n",
    "\n",
    "# Robust Scaling\n",
    "#scaler_robust = RobustScaler()\n",
    "#X_robust = scaler_robust.fit_transform(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
