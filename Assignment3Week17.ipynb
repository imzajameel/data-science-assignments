{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a09a49a-7389-45fa-8a1f-58153f6fa929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.17 \n",
    "#Assignment.3 \n",
    "#Question.1 : What is an ensemble technique in machine learning?\n",
    "#Answer.1 : # Ensemble Techniques in Machine Learning :\n",
    "\n",
    "# Ensemble techniques involve combining the predictions of multiple machine learning models to improve \n",
    "#overall performance. The idea is that combining diverse models can lead to better generalization and robustness \n",
    "#compared to individual models. Here are some common ensemble techniques:\n",
    "\n",
    "# 1. **Bagging (Bootstrap Aggregating):**\n",
    "#    - Bagging involves training multiple instances of the same base model on different subsets of the training data.\n",
    "#    - Random subsets are created by bootstrap sampling (sampling with replacement).\n",
    "#    - Example: Random Forest, where each tree in the forest is trained on a different subset of the data.\n",
    "\n",
    "# 2. **Boosting:**\n",
    "#    - Boosting focuses on improving the weaknesses of individual models by giving more weight to misclassified\n",
    "#instances.\n",
    "#    - Models are trained sequentially, and each new model corrects the errors of the previous one.\n",
    "#    - Example: AdaBoost, Gradient Boosting.\n",
    "\n",
    "# 3. **Stacking (Stacked Generalization):**\n",
    "#    - Stacking combines the predictions of multiple models by training a meta-model on their outputs.\n",
    "#    - Base models make individual predictions, and the meta-model learns to combine these predictions.\n",
    "#    - Example: StackingClassifier, StackingRegressor.\n",
    "\n",
    "# 4. **Voting:**\n",
    "#    - Voting involves combining predictions from multiple models by averaging (soft voting) or taking a \n",
    "#majority vote (hard voting).\n",
    "#    - Models can be of different types, and voting is applied to their individual predictions.\n",
    "#    - Example: VotingClassifier, VotingRegressor.\n",
    "\n",
    "# 5. **Random Forest:**\n",
    "#    - Random Forest is an ensemble of decision trees where each tree is trained on a random subset of features\n",
    "#and data.\n",
    "#    - It combines bagging and feature randomness to improve performance and reduce overfitting.\n",
    "\n",
    "# Ensemble techniques can be applied to various types of models, including classifiers and regressors, and they are\n",
    "#widely used to achieve better results in machine learning tasks.\n",
    "\n",
    "# Example Code (VotingClassifier):\n",
    "# ```python\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Load and split the data\n",
    "# X, y = load_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create individual classifiers\n",
    "# classifier1 = RandomForestClassifier()\n",
    "# classifier2 = GradientBoostingClassifier()\n",
    "# classifier3 = LogisticRegression()\n",
    "\n",
    "# # Create a VotingClassifier\n",
    "# voting_classifier = VotingClassifier(estimators=[\n",
    "#     ('rf', classifier1),\n",
    "#     ('gb', classifier2),\n",
    "#     ('lr', classifier3)\n",
    "# ], voting='hard')\n",
    "\n",
    "# # Train and predict with the ensemble model\n",
    "# voting_classifier.fit(X_train, y_train)\n",
    "# y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# # Evaluate the ensemble model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy of the VotingClassifier: {accuracy}\")\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8bc129-664b-4c07-9a73-b80c0fdcca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : Why are ensemble techniques used in machine learning?\n",
    "#Answer.2 : # Reasons for Using Ensemble Techniques in Machine Learning :\n",
    "\n",
    "# Ensemble techniques are widely used in machine learning for several reasons, leveraging the strengths \n",
    "#of multiple models to improve overall performance. Here are some key reasons for using ensemble techniques:\n",
    "\n",
    "# 1. **Improved Generalization:**\n",
    "#    - Combining predictions from multiple models helps reduce overfitting by capturing different aspects of \n",
    "#the underlying patterns in the data.\n",
    "#    - Ensemble models tend to generalize better to unseen data compared to individual models.\n",
    "\n",
    "# 2. **Increased Robustness:**\n",
    "#    - Ensembles are less sensitive to noise and outliers in the data since they aggregate information from\n",
    "#multiple models.\n",
    "#    - Outliers or misclassifications from one model may be compensated by correct predictions from others,\n",
    "#leading to a more robust overall performance.\n",
    "\n",
    "# 3. **Reduced Variance:**\n",
    "#    - Ensemble techniques, particularly bagging and boosting, can significantly reduce the variance of \n",
    "#individual models.\n",
    "#    - By training models on different subsets of the data or focusing on correcting errors, ensembles achieve\n",
    "#more stable and reliable predictions.\n",
    "\n",
    "# 4. **Handling Model Complexity:**\n",
    "#    - Ensembles are effective in handling complex relationships within the data.\n",
    "#    - Combining simple models (weak learners) can lead to a powerful ensemble capable of capturing intricate patterns.\n",
    "\n",
    "# 5. **Diverse Model Combination:**\n",
    "#    - Ensembles can leverage diverse models, each bringing a unique perspective to the problem.\n",
    "#    - Diversity in model types, hyperparameters, or training data subsets helps cover a broader range of scenarios.\n",
    "\n",
    "# 6. **Improved Accuracy:**\n",
    "#    - Ensembles often achieve higher accuracy compared to individual models.\n",
    "#    - By combining complementary strengths and mitigating weaknesses, ensembles can outperform their constituent models.\n",
    "\n",
    "# 7. **Flexibility Across Tasks:**\n",
    "#    - Ensemble techniques are versatile and can be applied to various machine learning tasks, including \n",
    "#classification, regression, and anomaly detection.\n",
    "#    - They can adapt to different types of data and model architectures.\n",
    "\n",
    "# 8. **Ease of Implementation:**\n",
    "#    - Many ensemble methods are readily available in machine learning libraries, making them easy to implement.\n",
    "#    - Libraries like scikit-learn provide ensemble classes for popular techniques like Random Forest, AdaBoost, \n",
    "#and VotingClassifier.\n",
    "\n",
    "# Overall, ensemble techniques are a powerful tool in the machine learning toolkit, offering improved\n",
    "#performance, robustness, and flexibility across a wide range of tasks.\n",
    "\n",
    "# Example Code (Random Forest):\n",
    "# ```python\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load and split the data\n",
    "# X, y = load_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a Random Forest classifier\n",
    "# random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train and predict with the Random Forest model\n",
    "# random_forest.fit(X_train, y_train)\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# # Evaluate the Random Forest model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy of the Random Forest: {accuracy}\")\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a8f100-3de3-4d27-b11b-8d1ffd091457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : What is bagging?\n",
    "#Answer.3 : # Bagging (Bootstrap Aggregating) in Machine Learning :\n",
    "\n",
    "# Bagging is an ensemble technique that involves training multiple instances of the same base model on \n",
    "#different subsets of the training data. The primary steps of bagging include:\n",
    "\n",
    "# 1. **Bootstrap Sampling:**\n",
    "#    - Randomly sample subsets of the training data with replacement.\n",
    "#    - Each subset (bootstrap sample) is of the same size as the original dataset but may contain duplicate instances.\n",
    "\n",
    "# 2. **Model Training:**\n",
    "#    - Train a base model on each bootstrap sample independently.\n",
    "#    - The base model can be any learning algorithm, and it is typically a weak learner, meaning it doesn't need \n",
    "#to be overly complex.\n",
    "\n",
    "# 3. **Aggregation (Averaging or Voting):**\n",
    "#    - Combine the predictions of individual models to obtain the final ensemble prediction.\n",
    "#    - For regression tasks, predictions are often averaged. For classification tasks, a majority vote is taken.\n",
    "\n",
    "# Key Characteristics of Bagging:\n",
    "# - **Diversity:** Bagging introduces diversity by training models on different subsets of the data, helping\n",
    "#to reduce overfitting.\n",
    "# - **Stability:** Bagging improves the stability and robustness of the model by averaging out the variance\n",
    "#associated with individual models.\n",
    "# - **Parallelization:** Training models on different subsets allows for parallelization, making bagging suitable \n",
    "#for distributed computing.\n",
    "\n",
    "# Example Code (Random Forest - Bagging for Decision Trees):\n",
    "# ```python\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load and split the data\n",
    "# X, y = load_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a Random Forest classifier (ensemble of decision trees)\n",
    "# random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train and predict with the Random Forest model\n",
    "# random_forest.fit(X_train, y_train)\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# # Evaluate the Random Forest model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy of the Random Forest (Bagging): {accuracy}\")\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93478e3-fe68-4c05-bee1-2e53c1ccb476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : What is boosting?\n",
    "#Answer.4 : # Boosting in Machine Learning :\n",
    "\n",
    "# Boosting is an ensemble technique that combines multiple weak learners to create a strong learner.\n",
    "#Unlike bagging, boosting focuses on sequentially improving the performance of the model by giving more weight \n",
    "#to instances that are misclassified. The key steps of boosting include:\n",
    "\n",
    "# 1. **Sequential Model Training:**\n",
    "#    - Train a series of weak learners sequentially, with each learner focusing on the mistakes made by the\n",
    "#previous ones.\n",
    "#    - Weak learners are typically simple models, such as shallow decision trees (stumps).\n",
    "\n",
    "# 2. **Instance Weighting:**\n",
    "#    - Assign weights to instances based on their performance in previous iterations.\n",
    "#    - Misclassified instances are given higher weights to prioritize learning from these mistakes.\n",
    "\n",
    "# 3. **Aggregation (Weighted Sum):**\n",
    "#    - Combine the predictions of individual models by assigning weights based on their performance.\n",
    "#    - The final prediction is often a weighted sum of the weak learners' predictions.\n",
    "\n",
    "# Key Characteristics of Boosting:\n",
    "# - **Focus on Errors:** Boosting aims to improve the model's performance by emphasizing instances that are \n",
    "#difficult to classify.\n",
    "# - **Sequential Learning:** Models are trained sequentially, with each iteration correcting the errors of the\n",
    "#previous ones.\n",
    "# - **Adaptive Weights:** Instances are assigned weights that adapt based on their classification errors, focusing \n",
    "#more on challenging instances.\n",
    "\n",
    "# Example Code (AdaBoost - Adaptive Boosting for Decision Stumps):\n",
    "# ```python\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load and split the data\n",
    "# X, y = load_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create an AdaBoost classifier (ensemble of decision stumps)\n",
    "# adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# # Train and predict with the AdaBoost model\n",
    "# adaboost.fit(X_train, y_train)\n",
    "# y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# # Evaluate the AdaBoost model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy of AdaBoost (Boosting): {accuracy}\")\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f341f3d-4249-4089-9ec8-4986545a80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : What are the benefits of using ensemble techniques?\n",
    "#Answer.5 : # Benefits of Ensemble Techniques in Machine Learning :\n",
    "\n",
    "# Ensemble techniques offer several advantages that contribute to their popularity in machine learning. Here \n",
    "#are some key benefits:\n",
    "\n",
    "# 1. **Improved Accuracy:**\n",
    "#    - Ensembles often result in higher predictive accuracy compared to individual models.\n",
    "#    - Combining diverse models helps mitigate the weaknesses of individual models, leading to more robust predictions.\n",
    "\n",
    "# 2. **Reduction of Overfitting:**\n",
    "#    - Ensembles, particularly bagging techniques like Random Forest, can reduce overfitting by aggregating \n",
    "#predictions from multiple models.\n",
    "#    - Overfitting tendencies in individual models may be counteracted by combining their predictions.\n",
    "\n",
    "# 3. **Enhanced Stability and Robustness:**\n",
    "#    - Ensembles are more stable and robust, as they are less sensitive to variations in the training data.\n",
    "#    - Variability and uncertainty associated with single models are mitigated by combining predictions.\n",
    "\n",
    "# 4. **Handling Complexity:**\n",
    "#    - Ensembles can effectively handle complex relationships and capture patterns that may be challenging\n",
    "#for individual models.\n",
    "#    - Boosting techniques, in particular, focus on improving the model's performance on difficult instances.\n",
    "\n",
    "# 5. **Versatility:**\n",
    "#    - Ensemble methods are versatile and can be applied to various types of base models.\n",
    "#    - They can be used for both classification and regression tasks with different underlying algorithms.\n",
    "\n",
    "# 6. **Parallelization and Scalability:**\n",
    "#    - Bagging techniques, such as Random Forest, are inherently parallelizable, making them suitable for\n",
    "#distributed computing.\n",
    "#    - Ensembles can be scaled to handle large datasets and complex problems.\n",
    "\n",
    "# Example Code (Random Forest for Classification):\n",
    "# ```python\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load and split the data\n",
    "# X, y = load_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a Random Forest classifier (ensemble of decision trees)\n",
    "# random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train and predict with the Random Forest model\n",
    "# random_forest.fit(X_train, y_train)\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# # Evaluate the Random Forest model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy of the Random Forest: {accuracy}\")\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5e9a35-813d-431a-91ff-4e088f4072b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : Are ensemble techniques always better than individual models?\n",
    "#Answer.6 : # Ensemble Techniques vs. Individual Models in Machine Learning in Python Comments:\n",
    "\n",
    "# The effectiveness of ensemble techniques compared to individual models depends on several factors. Here are some\n",
    "#considerations:\n",
    "\n",
    "# 1. **Diversity of Models:**\n",
    "#    - Ensembles benefit from combining diverse models that have different strengths and weaknesses.\n",
    "#    - If individual models are too similar, the ensemble might not provide significant improvements.\n",
    "\n",
    "# 2. **Size of Dataset:**\n",
    "#    - In smaller datasets, individual models might be prone to overfitting, and ensembles can help mitigate this issue.\n",
    "#    - For large datasets, individual models may already generalize well, and the improvement gained by ensembles\n",
    "#may be marginal.\n",
    "\n",
    "# 3. **Complexity of the Problem:**\n",
    "#    - Ensembles are particularly effective when dealing with complex relationships and challenging patterns.\n",
    "#    - For simple problems, individual models might perform well, and the added complexity of an ensemble may not \n",
    "#be necessary.\n",
    "\n",
    "# 4. **Computational Resources:**\n",
    "#    - Ensembles, especially those with a large number of models (e.g., Random Forest), can be computationally expensive.\n",
    "#    - If computational resources are limited, using a single well-tuned model might be more practical.\n",
    "\n",
    "# 5. **Model Interpretability:**\n",
    "#    - Individual models are often easier to interpret compared to ensembles.\n",
    "#    - If interpretability is crucial, a single model might be preferred, especially in domains with strict\n",
    "#regulatory requirements.\n",
    "\n",
    "# 6. **Training Time:**\n",
    "#    - Ensembles, especially boosting algorithms, are trained sequentially and can be time-consuming.\n",
    "#    - Individual models may offer faster training times, which is essential in scenarios where quick model \n",
    "#deployment is necessary.\n",
    "\n",
    "# Example Code (Comparing Ensemble and Individual Models):\n",
    "# ```python\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load and split the data\n",
    "# X, y = load_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Individual Model (Logistic Regression)\n",
    "# lr_model = LogisticRegression()\n",
    "# lr_model.fit(X_train, y_train)\n",
    "# y_lr_pred = lr_model.predict(X_test)\n",
    "# lr_accuracy = accuracy_score(y_test, y_lr_pred)\n",
    "\n",
    "# # Ensemble Model (Random Forest)\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# y_rf_pred = rf_model.predict(X_test)\n",
    "# rf_accuracy = accuracy_score(y_test, y_rf_pred)\n",
    "\n",
    "# print(f\"Accuracy of Logistic Regression: {lr_accuracy}\")\n",
    "# print(f\"Accuracy of Random Forest (Ensemble): {rf_accuracy}\")\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12be22d-415c-4f6b-a1d7-3388c752d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : How is the confidence interval calculated using bootstrap?\n",
    "#Answer.7 : # Confidence Interval Calculation using Bootstrap in Python Comments:\n",
    "\n",
    "# Bootstrap resampling is a technique for estimating the distribution of a statistic by repeatedly resampling\n",
    "#with replacement from the observed data. Confidence intervals can be derived from the distribution of the \n",
    "#resampled statistic. Here's how you can calculate a confidence interval using bootstrap in Python:\n",
    "\n",
    "# Required Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Function to Generate Bootstrap Samples\n",
    "def generate_bootstrap_samples(data, num_samples):\n",
    "    \"\"\"\n",
    "    Generate bootstrap samples from the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array or list, the original data\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "\n",
    "    Returns:\n",
    "    - bootstrap_samples: list of numpy arrays, the generated bootstrap samples\n",
    "    \"\"\"\n",
    "    num_data_points = len(data)\n",
    "    bootstrap_samples = [np.random.choice(data, num_data_points, replace=True) for _ in range(num_samples)]\n",
    "    return bootstrap_samples\n",
    "\n",
    "# Function to Calculate Statistic of Interest\n",
    "def calculate_statistic(data):\n",
    "    \"\"\"\n",
    "    Calculate the statistic of interest from the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array or list, the data for which the statistic is calculated\n",
    "\n",
    "    Returns:\n",
    "    - statistic: float, the calculated statistic\n",
    "    \"\"\"\n",
    "    # Example: Mean as the statistic\n",
    "    return np.mean(data)\n",
    "\n",
    "# Function to Calculate Bootstrap Confidence Interval\n",
    "def calculate_bootstrap_ci(data, num_samples, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the bootstrap confidence interval for a given data and statistic.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array or list, the original data\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "    - alpha: float, the significance level (e.g., 0.05 for a 95% confidence interval)\n",
    "\n",
    "    Returns:\n",
    "    - ci_lower: float, the lower bound of the confidence interval\n",
    "    - ci_upper: float, the upper bound of the confidence interval\n",
    "    \"\"\"\n",
    "    # Generate bootstrap samples\n",
    "    bootstrap_samples = generate_bootstrap_samples(data, num_samples)\n",
    "\n",
    "    # Calculate the statistic for each bootstrap sample\n",
    "    bootstrap_statistics = [calculate_statistic(sample) for sample in bootstrap_samples]\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    alpha_percentile = 100 * (alpha / 2)\n",
    "    ci_lower = np.percentile(bootstrap_statistics, alpha_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_statistics, 100 - alpha_percentile)\n",
    "\n",
    "    return ci_lower, ci_upper\n",
    "\n",
    "# Example Usage\n",
    "# data = your_data_here\n",
    "# num_bootstrap_samples = your_desired_number_of_samples\n",
    "# alpha_level = your_desired_significance_level\n",
    "\n",
    "# ci_lower, ci_upper = calculate_bootstrap_ci(data, num_bootstrap_samples, alpha_level)\n",
    "# print(f\"Bootstrap Confidence Interval: [{ci_lower}, {ci_upper}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d606aa-3276-42ae-a7fc-6e4d784a226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.8 : How does bootstrap work and What are the steps involved in bootstrap?\n",
    "#Answer.8 : # Bootstrap Resampling :\n",
    "\n",
    "# Bootstrap resampling is a statistical technique used to estimate the sampling distribution of a statistic\n",
    "#by repeatedly resampling with replacement from the observed data. Here are the steps involved in performing \n",
    "#bootstrap resampling:\n",
    "\n",
    "# Required Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the Original Dataset\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Step 2: Specify the Number of Bootstrap Samples to Generate\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "# Step 3: Generate Bootstrap Samples\n",
    "def generate_bootstrap_samples(data, num_samples):\n",
    "    \"\"\"\n",
    "    Generate bootstrap samples from the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array or list, the original data\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "\n",
    "    Returns:\n",
    "    - bootstrap_samples: list of numpy arrays, the generated bootstrap samples\n",
    "    \"\"\"\n",
    "    num_data_points = len(data)\n",
    "    bootstrap_samples = [np.random.choice(data, num_data_points, replace=True) for _ in range(num_samples)]\n",
    "    return bootstrap_samples\n",
    "\n",
    "bootstrap_samples = generate_bootstrap_samples(original_data, num_bootstrap_samples)\n",
    "\n",
    "# Step 4: Calculate the Statistic of Interest for Each Bootstrap Sample\n",
    "def calculate_statistic(data):\n",
    "    \"\"\"\n",
    "    Calculate the statistic of interest from the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array or list, the data for which the statistic is calculated\n",
    "\n",
    "    Returns:\n",
    "    - statistic: float, the calculated statistic\n",
    "    \"\"\"\n",
    "    # Example: Mean as the statistic\n",
    "    return np.mean(data)\n",
    "\n",
    "bootstrap_statistics = [calculate_statistic(sample) for sample in bootstrap_samples]\n",
    "\n",
    "# Step 5: Analyze the Distribution of Bootstrap Statistics\n",
    "# (Optional) Visualize the distribution, calculate confidence intervals, etc.\n",
    "\n",
    "# Example Usage\n",
    "# - Use the bootstrap_statistics for further analysis or visualization.\n",
    "\n",
    "# Note: The steps mentioned here provide a high-level overview of the bootstrap resampling process.\n",
    "# Depending on the specific use case, additional considerations such as bias correction may be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829c9d0-2496-40c6-a210-47fd59fec4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.9 : A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "#sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "#bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "#Answer.9 : # Bootstrap Resampling for Confidence Interval Calculation :\n",
    "\n",
    "# Required Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the Sample Data\n",
    "sample_heights = np.array([15] * 50)  # Example: Sample mean height of 15 meters\n",
    "\n",
    "# Step 2: Specify the Number of Bootstrap Samples to Generate\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Step 3: Generate Bootstrap Samples\n",
    "def generate_bootstrap_samples(data, num_samples):\n",
    "    \"\"\"\n",
    "    Generate bootstrap samples from the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array or list, the original data\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "\n",
    "    Returns:\n",
    "    - bootstrap_samples: list of numpy arrays, the generated bootstrap samples\n",
    "    \"\"\"\n",
    "    num_data_points = len(data)\n",
    "    bootstrap_samples = [np.random.choice(data, num_data_points, replace=True) for _ in range(num_samples)]\n",
    "    return bootstrap_samples\n",
    "\n",
    "bootstrap_samples = generate_bootstrap_samples(sample_heights, num_bootstrap_samples)\n",
    "\n",
    "# Step 4: Calculate the Mean for Each Bootstrap Sample\n",
    "bootstrap_means = [np.mean(sample) for sample in bootstrap_samples]\n",
    "\n",
    "# Step 5: Calculate the Confidence Interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Step 6: Print the Result\n",
    "print(f\"Bootstrap Confidence Interval for Mean Height: {confidence_interval} meters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
