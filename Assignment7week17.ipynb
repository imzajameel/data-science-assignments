{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1769c0-7dc9-49ff-b8d6-45513a76541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.17 \n",
    "#Assignment.7 \n",
    "#Question.1 : What is boosting in machine learning?\n",
    "#Answer.1 : # Boosting in machine learning is an ensemble learning technique that combines the predictions\n",
    "# of multiple weak learners to create a strong learner with improved accuracy and predictive performance.\n",
    "\n",
    "# 1. Sequential Training:\n",
    "#    Boosting trains a sequence of weak learners, where each new learner focuses on the mistakes\n",
    "#    made by the combination of the existing learners.\n",
    "\n",
    "# 2. Weighted Voting:\n",
    "#    The weak learners are combined through a weighted voting mechanism. Each weak learner is assigned\n",
    "#    a weight based on its performance, and the final prediction is obtained by combining the predictions\n",
    "#    of all weak learners with their respective weights.\n",
    "\n",
    "# 3. Adaptive Learning:\n",
    "#    Boosting is an adaptive learning method where the weight of each observation is adjusted based on\n",
    "#    the errors made by the previous weak learners. Misclassified instances receive higher weights,\n",
    "#    and subsequent weak learners focus more on these instances during training.\n",
    "\n",
    "# 4. Iterative Process:\n",
    "#    Boosting is an iterative process where each new weak learner is added to the ensemble to correct\n",
    "#    the errors made by the existing ones. The process continues until a predefined number of weak\n",
    "#    learners is reached or until a certain level of accuracy is achieved.\n",
    "\n",
    "# 5. Common Algorithms:\n",
    "#    - AdaBoost (Adaptive Boosting): Adjusts the weights of misclassified instances.\n",
    "#    - Gradient Boosting: Fits each new model to the residuals (errors) of the combined ensemble.\n",
    "\n",
    "# Boosting is effective in improving model accuracy, especially for complex and noisy datasets.\n",
    "# It is widely used in various applications, including classification and regression tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789ab1af-227a-4995-831b-7a3f293ce01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : What are the advantages and limitations of using boosting techniques?\n",
    "#Answer.2 : # Advantages of Boosting Techniques:\n",
    "\n",
    "# 1. Improved Accuracy:\n",
    "#    - Boosting combines multiple weak learners to create a strong learner, leading to higher accuracy.\n",
    "\n",
    "# 2. Handling Noisy Data:\n",
    "#    - Boosting is robust to noisy data and outliers, as the adaptive learning process focuses on instances\n",
    "#with higher errors.\n",
    "\n",
    "# 3. Feature Importance:\n",
    "#    - Boosting algorithms provide insights into feature importance by analyzing the weights assigned \n",
    "#during the boosting process.\n",
    "\n",
    "# 4. Versatility:\n",
    "#    - Boosting is versatile and applicable to various data types and tasks, including classification and regression.\n",
    "\n",
    "# 5. No Overfitting as Easily:\n",
    "#    - Properly tuned boosting algorithms are less prone to overfitting, with mechanisms like shrinkage to prevent it.\n",
    "\n",
    "# Limitations of Boosting Techniques:\n",
    "\n",
    "# 1. Sensitivity to Noisy Data:\n",
    "#    - While generally robust, extremely noisy observations can impact performance.\n",
    "\n",
    "# 2. Computationally Intensive:\n",
    "#    - Boosting can be computationally intensive, especially with a large number of weak learners.\n",
    "\n",
    "# 3. Overfitting if Not Properly Tuned:\n",
    "#    - Without proper tuning, boosting can still overfit; parameters like learning rate and weak learner count need \n",
    "#adjustment.\n",
    "\n",
    "# 4. Less Interpretability:\n",
    "#    - The combined model is often complex, making it less interpretable compared to simpler models.\n",
    "\n",
    "# 5. Potential for Bias:\n",
    "#    - Boosting may introduce bias if weak learners are too specialized, especially with imbalanced datasets.\n",
    "\n",
    "# It's crucial to carefully consider these factors and tune hyperparameters to ensure optimal performance when using\n",
    "#boosting techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef32744a-eb33-47f7-8c77-fe9e4de24e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : Explain how boosting works.\n",
    "#Answer.3 : # Overview of Boosting:\n",
    "# Boosting combines predictions of weak learners to form a strong learner.\n",
    "# Sequential training with increased focus on misclassified instances.\n",
    "\n",
    "# Steps in Boosting:\n",
    "\n",
    "# 1. Initialization:\n",
    "#    - Equal weights assigned to all training instances.\n",
    "#    - Start with a weak learner (e.g., decision stump).\n",
    "\n",
    "# 2. Training Weak Learners:\n",
    "#    - Train a weak learner on the dataset with assigned weights.\n",
    "#    - Weak learners are simple models with limited predictive power.\n",
    "\n",
    "# 3. Compute Error:\n",
    "#    - Calculate the error of the weak learner on the training set.\n",
    "#    - Higher weight to misclassified instances.\n",
    "\n",
    "# 4. Compute Learner Weight:\n",
    "#    - Calculate the weight of the weak learner based on its accuracy.\n",
    "#    - More accurate learners get higher weights.\n",
    "\n",
    "# 5. Update Weights:\n",
    "#    - Adjust weights of training instances.\n",
    "#    - Increase weights of misclassified instances.\n",
    "\n",
    "# 6. Repeat:\n",
    "#    - Repeat steps 2-5 for a defined number of iterations or stopping criterion.\n",
    "\n",
    "# 7. Combine Weak Learners:\n",
    "#    - Combine predictions of weak learners with their respective weights.\n",
    "#    - Formulate a strong learner that is a weighted sum of weak learners.\n",
    "\n",
    "# 8. Final Model:\n",
    "#    - The final boosted model is a combination of weak learners, each contributing to the overall prediction.\n",
    "\n",
    "# AdaBoost (Adaptive Boosting):\n",
    "# - AdaBoost adjusts weights of misclassified instances.\n",
    "# - Correctly classified instances receive lower weights.\n",
    "# - Iterative learning process focuses on challenging instances.\n",
    "# - Final model is a combination of weak learners with weighted predictions.\n",
    "\n",
    "# Gradient Boosting:\n",
    "# - Generalization of boosting that includes AdaBoost.\n",
    "# - Minimizes a cost function using gradient descent.\n",
    "# - Each new weak learner corrects errors of the combined model.\n",
    "# - Shrinkage parameter controls the contribution of each weak learner.\n",
    "# - Regularization terms can be included.\n",
    "\n",
    "# Benefits of Boosting:\n",
    "# - Improved accuracy by focusing on challenging instances.\n",
    "# - Versatile for various data types and tasks.\n",
    "# - Robust to noisy data and outliers.\n",
    "\n",
    "# Considerations:\n",
    "# - Hyperparameter tuning is crucial for optimal performance.\n",
    "# - Choice of weak learner influences boosting performance.\n",
    "# - Techniques like early stopping and regularization help avoid overfitting.\n",
    "\n",
    "# Example in Python:\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset and split into features and labels\n",
    "#X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize AdaBoostClassifier\n",
    "#ada_boost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "#ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "#predictions = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "#accuracy = accuracy_score(y_test, predictions)\n",
    "#print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9330f1-745a-4cd5-8d46-6191201d7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : What are the different types of boosting algorithms?\n",
    "#Answer.4 : # Types of Boosting Algorithms:\n",
    "\n",
    "# 1. AdaBoost (Adaptive Boosting):\n",
    "#    - Emphasizes misclassified instances by assigning higher weights.\n",
    "#    - Iteratively trains weak learners and adjusts weights.\n",
    "#    - Final model is a weighted sum of weak learners.\n",
    "\n",
    "# 2. Gradient Boosting:\n",
    "#    - Generalization of boosting that includes AdaBoost.\n",
    "#    - Minimizes a cost function using gradient descent.\n",
    "#    - Each new weak learner corrects errors of the combined model.\n",
    "#    - Shrinkage parameter controls the contribution of each weak learner.\n",
    "\n",
    "# 3. XGBoost (Extreme Gradient Boosting):\n",
    "#    - Efficient and scalable implementation of gradient boosting.\n",
    "#    - Parallel computing and regularization techniques.\n",
    "#    - Tree pruning and cross-validation for optimal tree depth.\n",
    "\n",
    "# 4. LightGBM (Light Gradient Boosting Machine):\n",
    "#    - Gradient boosting framework designed for distributed computing.\n",
    "#    - Efficient handling of large datasets and high-dimensional features.\n",
    "#    - Leaf-wise tree growth strategy for faster convergence.\n",
    "\n",
    "# 5. CatBoost:\n",
    "#    - Gradient boosting algorithm specifically designed for categorical features.\n",
    "#    - Automatically handles categorical data encoding.\n",
    "#    - Robust to overfitting and requires minimal hyperparameter tuning.\n",
    "\n",
    "# 6. Stochastic Gradient Boosting:\n",
    "#    - Introduces randomness in the training process.\n",
    "#    - Randomly selects a subset of instances for each iteration.\n",
    "#    - Reduces overfitting and enhances generalization.\n",
    "\n",
    "# 7. LGBM (Light Gradient Boosting Machine):\n",
    "#    - Similar to LightGBM, optimized for speed and efficiency.\n",
    "#    - Uses histogram-based learning for faster computation.\n",
    "#    - Suitable for large datasets and distributed computing.\n",
    "\n",
    "# 8. LogitBoost:\n",
    "#    - Boosting algorithm designed for binary classification.\n",
    "#    - Minimizes logistic loss function.\n",
    "#    - Adds new weak learners based on minimizing the pseudo-residuals.\n",
    "\n",
    "# 9. BrownBoost:\n",
    "#    - An extension of AdaBoost with a different weighting scheme.\n",
    "#    - Utilizes information gain and penalizes false positives and negatives.\n",
    "#    - Aims to address the limitations of AdaBoost.\n",
    "\n",
    "# Note: Each boosting algorithm has its strengths and weaknesses, and the choice depends on the specific \n",
    "#characteristics of the dataset and the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97fee0f-9540-4db7-a84e-4cb3539b9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : What are some common parameters in boosting algorithms?\n",
    "#Answer.5 : # Common Parameters in Boosting Algorithms:\n",
    "\n",
    "# 1. n_estimators:\n",
    "#    - Number of weak learners (trees) to train.\n",
    "#    - Higher values may lead to better performance but increased computation time.\n",
    "\n",
    "# 2. learning_rate (or shrinkage):\n",
    "#    - Determines the contribution of each weak learner to the final model.\n",
    "#    - Smaller values require more weak learners for comparable performance.\n",
    "\n",
    "# 3. max_depth:\n",
    "#    - Maximum depth of each weak learner (tree).\n",
    "#    - Controls the complexity of weak learners and helps prevent overfitting.\n",
    "\n",
    "# 4. subsample:\n",
    "#    - Fraction of training instances randomly selected for each weak learner.\n",
    "#    - Introduces randomness and helps prevent overfitting.\n",
    "\n",
    "# 5. min_samples_split:\n",
    "#    - Minimum number of samples required to split a node.\n",
    "#    - Controls the granularity of tree nodes and influences model complexity.\n",
    "\n",
    "# 6. loss (for Gradient Boosting):\n",
    "#    - Specifies the loss function to be minimized during training.\n",
    "#    - Common options include 'deviance' for logistic regression and 'ls' for least squares regression.\n",
    "\n",
    "# 7. colsample_bytree (for XGBoost):\n",
    "#    - Fraction of features randomly selected for each tree.\n",
    "#    - Introduces feature randomness and aids in preventing overfitting.\n",
    "\n",
    "# 8. reg_alpha and reg_lambda (for XGBoost):\n",
    "#    - Regularization terms to control overfitting.\n",
    "#    - reg_alpha adds L1 regularization, and reg_lambda adds L2 regularization.\n",
    "\n",
    "# 9. scale_pos_weight (for imbalanced datasets):\n",
    "#    - Adjusts the balance of positive and negative class weights.\n",
    "#    - Useful when dealing with imbalanced binary classification problems.\n",
    "\n",
    "# 10. categorical_feature (for CatBoost):\n",
    "#     - Specifies the indices of categorical features in the dataset.\n",
    "#     - CatBoost automatically handles categorical data, but specifying this parameter can improve performance.\n",
    "\n",
    "# Note: The significance and optimal values of these parameters can vary across different boosting algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74418e7f-cde9-4312-9a99-bfe0eaf8eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : How do boosting algorithms combine weak learners to create a strong learner?\n",
    "#Answer.6 : # Combining Weak Learners in Boosting Algorithms:\n",
    "\n",
    "# 1. AdaBoost (Adaptive Boosting):\n",
    "#    - Assigns weights to each training instance.\n",
    "#    - Iteratively trains weak learners, adjusting weights based on misclassifications.\n",
    "#    - Combines weak learners by assigning higher weights to correctly classified instances.\n",
    "\n",
    "# 2. Gradient Boosting:\n",
    "#    - Trains a sequence of weak learners, each correcting errors of the previous ones.\n",
    "#    - Constructs an additive model where each weak learner contributes to the final prediction.\n",
    "#    - The prediction of the combined model is the sum of predictions from individual weak learners.\n",
    "\n",
    "# 3. XGBoost (Extreme Gradient Boosting):\n",
    "#    - Employs a gradient descent optimization approach.\n",
    "#    - Iteratively fits weak learners to the negative gradient of the loss function.\n",
    "#    - Calculates the prediction as the sum of contributions from individual weak learners.\n",
    "\n",
    "# 4. LightGBM (Light Gradient Boosting Machine):\n",
    "#    - Utilizes a histogram-based approach for efficient computation.\n",
    "#    - Builds trees in a leaf-wise manner, selecting the leaf with the maximum gain.\n",
    "#    - Sum of leaf values contributes to the final prediction.\n",
    "\n",
    "# 5. CatBoost:\n",
    "#    - Constructs an ensemble of decision trees with categorical feature handling.\n",
    "#    - Adapts weights during training to prioritize instances with larger gradients.\n",
    "#    - Aggregates the predictions of individual trees with adjusted weights.\n",
    "\n",
    "# 6. Stochastic Gradient Boosting:\n",
    "#    - Introduces randomness by using random subsets of instances for training.\n",
    "#    - Each weak learner contributes to the final model by adjusting predictions.\n",
    "\n",
    "# 7. LogitBoost:\n",
    "#    - Focuses on minimizing logistic loss.\n",
    "#    - Adds weak learners sequentially, adjusting weights based on log-odds.\n",
    "\n",
    "# 8. BrownBoost:\n",
    "#    - Extends AdaBoost with a different weighting scheme.\n",
    "#    - Adjusts weights based on misclassifications and false positives/negatives.\n",
    "\n",
    "# The fundamental idea in boosting is to combine the predictions of multiple weak learners to form a more accurate \n",
    "#and robust model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab18587-b5c8-4ccd-95ab-aeb2a118b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : Explain the concept of AdaBoost algorithm and its working.\n",
    "#Answer.7 : # AdaBoost (Adaptive Boosting) Algorithm:\n",
    "\n",
    "# Concept:\n",
    "# - AdaBoost is an ensemble learning algorithm that combines multiple weak learners to create a strong learner.\n",
    "# - Weak learners are typically simple models that perform slightly better than random guessing.\n",
    "# - The algorithm assigns weights to training instances, emphasizing the misclassified instances in subsequent\n",
    "#iterations.\n",
    "\n",
    "# Working Steps:\n",
    "\n",
    "# 1. Assign Equal Weights:\n",
    "#    - Initially, all training instances are assigned equal weights.\n",
    "\n",
    "# 2. Train Weak Learner:\n",
    "#    - A weak learner (e.g., a decision stump) is trained on the dataset, and its predictions are evaluated.\n",
    "#    - Instances that are misclassified receive higher weights, and correctly classified instances receive lower weights.\n",
    "\n",
    "# 3. Calculate Error:\n",
    "#    - Calculate the weighted error (weighted sum of misclassified instance weights) of the weak learner.\n",
    "\n",
    "# 4. Compute Weak Learner Weight:\n",
    "#    - Compute the weight of the weak learner in the final model based on its error rate.\n",
    "#    - A lower error rate results in a higher weight.\n",
    "\n",
    "# 5. Update Instance Weights:\n",
    "#    - Update the weights of training instances.\n",
    "#    - Increase the weights of misclassified instances, making them more influential in the next iteration.\n",
    "\n",
    "# 6. Repeat:\n",
    "#    - Repeat steps 2-5 for a specified number of iterations or until a predefined accuracy is achieved.\n",
    "\n",
    "# 7. Final Prediction:\n",
    "#    - Combine the predictions of all weak learners with their respective weights to form the final strong learner.\n",
    "\n",
    "# Key Characteristics:\n",
    "# - Weights are adjusted in each iteration to focus on misclassified instances.\n",
    "# - Each weak learner corrects errors made by the previous ones.\n",
    "# - Final model is an additive combination of weak learners with higher accuracy on difficult instances.\n",
    "\n",
    "# AdaBoost is effective in improving the accuracy of weak models and handling complex datasets with varied patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378f6040-6b48-4414-8cbf-357a403d086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.8 : What is the loss function used in AdaBoost algorithm?\n",
    "#Answer.8 : # Loss Function in AdaBoost:\n",
    "\n",
    "# The loss function used in AdaBoost is the Exponential Loss (also known as the AdaBoost Loss).\n",
    "# It is defined as follows:\n",
    "\n",
    "# Exponential Loss (AdaBoost Loss):\n",
    "# L(y, f(x)) = exp(-y * f(x))\n",
    "\n",
    "# Where:\n",
    "# - y is the true label (-1 or 1 for binary classification).\n",
    "# - f(x) is the combined prediction of the weak learners.\n",
    "\n",
    "# The goal of AdaBoost is to minimize the exponential loss, encouraging the model to focus on instances\n",
    "# that are misclassified by the current ensemble of weak learners. Instances with higher weights (misclassified)\n",
    "# contribute more to the loss, guiding subsequent weak learners to correct these mistakes.\n",
    "\n",
    "# The exponential loss is well-suited for boosting algorithms as it strongly penalizes misclassifications,\n",
    "# emphasizing the importance of difficult-to-classify instances in the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f7191f-6e40-4846-b666-9761eea354fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.9 : How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "#Answer.9 : # AdaBoost (Adaptive Boosting) Algorithm:\n",
    "\n",
    "# Concept:\n",
    "# - AdaBoost is an ensemble learning algorithm that combines multiple weak learners to create a strong learner.\n",
    "# - Weak learners are typically simple models that perform slightly better than random guessing.\n",
    "# - The algorithm assigns weights to training instances, emphasizing the misclassified instances in subsequent\n",
    "#iterations.\n",
    "\n",
    "# Working Steps:\n",
    "\n",
    "# 1. Assign Equal Weights:\n",
    "#    - Initially, all training instances are assigned equal weights.\n",
    "\n",
    "# 2. Train Weak Learner:\n",
    "#    - A weak learner (e.g., a decision stump) is trained on the dataset, and its predictions are evaluated.\n",
    "#    - Instances that are misclassified receive higher weights, and correctly classified instances receive lower weights.\n",
    "\n",
    "# 3. Calculate Error:\n",
    "#    - Calculate the weighted error (weighted sum of misclassified instance weights) of the weak learner.\n",
    "\n",
    "# 4. Compute Weak Learner Weight:\n",
    "#    - Compute the weight of the weak learner in the final model based on its error rate.\n",
    "#    - A lower error rate results in a higher weight.\n",
    "\n",
    "# 5. Update Instance Weights:\n",
    "#    - Update the weights of training instances.\n",
    "#    - Increase the weights of misclassified instances, making them more influential in the next iteration.\n",
    "\n",
    "# 6. Repeat:\n",
    "#    - Repeat steps 2-5 for a specified number of iterations or until a predefined accuracy is achieved.\n",
    "\n",
    "# 7. Final Prediction:\n",
    "#    - Combine the predictions of all weak learners with their respective weights to form the final strong learner.\n",
    "\n",
    "# Key Characteristics:\n",
    "# - Weights are adjusted in each iteration to focus on misclassified instances.\n",
    "# - Each weak learner corrects errors made by the previous ones.\n",
    "# - Final model is an additive combination of weak learners with higher accuracy on difficult instances.\n",
    "\n",
    "# AdaBoost is effective in improving the accuracy of weak models and handling complex datasets with varied patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e95dfd-65a0-4383-bfdb-1dd49a926548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.10 : What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "#Answer.10 : \n",
    "# Effect of Increasing Estimators in AdaBoost:\n",
    "\n",
    "# The number of estimators in AdaBoost corresponds to the number of weak learners (e.g., decision stumps)\n",
    "# that are sequentially trained and combined to form the final strong learner. Increasing the number of estimators\n",
    "# has the following effects:\n",
    "\n",
    "# 1. Improved Training Performance:\n",
    "#    - Initially, as more weak learners are added, the algorithm may fit the training data more closely.\n",
    "#    - The ensemble becomes more capable of capturing complex patterns in the training set.\n",
    "\n",
    "# 2. Decreased Training Error:\n",
    "#    - With more estimators, AdaBoost is likely to reduce the training error further.\n",
    "#    - The ensemble becomes better at correcting errors made by previous weak learners.\n",
    "\n",
    "# 3. Potential Overfitting:\n",
    "#    - Beyond a certain point, increasing the number of estimators may lead to overfitting, especially if the\n",
    "#      dataset is not sufficiently complex.\n",
    "#    - The model may start memorizing the training data, resulting in reduced generalization performance on\n",
    "#      unseen data.\n",
    "\n",
    "# 4. Increased Computational Cost:\n",
    "#    - Training more weak learners increases the computational cost of the algorithm.\n",
    "#    - There is a trade-off between improved performance and computational efficiency.\n",
    "\n",
    "# 5. Balancing Act:\n",
    "#    - The optimal number of estimators depends on the dataset and problem complexity.\n",
    "#    - It is recommended to use techniques such as cross-validation to find the optimal number that balances\n",
    "#      model performance and generalization.\n",
    "\n",
    "# In summary, increasing the number of estimators can enhance the model's capacity to learn from the data,\n",
    "# but careful consideration is needed to avoid overfitting and unnecessary computational cost.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
