{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5829d7-6543-42dc-8323-546a7a586eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.17 \n",
    "#Assignment.2 \n",
    "#Question.1 : A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "#company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "#probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "#Answer.1 : # Probability Calculation : \n",
    "\n",
    "# Given Data:\n",
    "# - P(Health Insurance Plan) = 70%\n",
    "# - P(Smoker | Health Insurance Plan) = 40%\n",
    "\n",
    "# Calculate Probability of being a smoker given the use of the health insurance plan using Bayes' Theorem:\n",
    "\n",
    "# Bayes' Theorem:\n",
    "# P(Smoker | Health Insurance Plan) = P(Health Insurance Plan | Smoker) * P(Smoker) / P(Health Insurance Plan)\n",
    "\n",
    "# Assume:\n",
    "# - P(Smoker) = Prior probability of being a smoker (not given, let's assume 50% for illustration purposes)\n",
    "\n",
    "# Calculate P(Health Insurance Plan):\n",
    "P_Health_Insurance_Plan = 0.7\n",
    "\n",
    "# Calculate P(Smoker):\n",
    "P_Smoker = 0.5  # Assumption for illustration\n",
    "\n",
    "# Calculate P(Health Insurance Plan | Smoker):\n",
    "P_Health_Insurance_Plan_given_Smoker = 0.4\n",
    "\n",
    "# Apply Bayes' Theorem:\n",
    "P_Smoker_given_Health_Insurance_Plan = (P_Health_Insurance_Plan_given_Smoker * P_Smoker) / P_Health_Insurance_Plan\n",
    "\n",
    "# The result represents the probability of an employee being a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "# Example Code (Probability Calculation):\n",
    "# ```python\n",
    "# # Given data\n",
    "# P_Health_Insurance_Plan = 0.7\n",
    "# P_Smoker = 0.5  # Assumption for illustration\n",
    "# P_Health_Insurance_Plan_given_Smoker = 0.4\n",
    "\n",
    "# # Bayes' Theorem\n",
    "# P_Smoker_given_Health_Insurance_Plan = (P_Health_Insurance_Plan_given_Smoker * P_Smoker) / P_Health_Insurance_Plan\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36694bec-0b75-4282-bbaf-150d3554b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "#Answer.2 : # Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes :\n",
    "\n",
    "# Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of Naive Bayes classifiers, but they \n",
    "#differ in their assumptions and the type of data they are suitable for.\n",
    "\n",
    "# 1. **Bernoulli Naive Bayes:**\n",
    "#    - Assumes that features are binary (0 or 1).\n",
    "#    - Typically used for binary or boolean features.\n",
    "#    - Commonly employed in text classification problems where the presence or absence of words (features) is considered.\n",
    "\n",
    "# 2. **Multinomial Naive Bayes:**\n",
    "#    - Assumes that features represent counts or frequencies of events (integer values).\n",
    "#    - Suitable for problems with discrete features, often used in text classification with bag-of-words representation.\n",
    "#    - Handles multiple occurrences of the same feature in a document.\n",
    "\n",
    "# Both classifiers are based on the Naive Bayes assumption of feature independence given the class label. \n",
    "#The choice between them depends on the nature of the features and the problem at hand.\n",
    "\n",
    "# Example Code (Usage of Bernoulli and Multinomial Naive Bayes):\n",
    "# ```python\n",
    "# from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Example data\n",
    "# X_text = [\"document1\", \"document2\", ...]  # List of documents\n",
    "# y_labels = [0, 1, ...]  # Corresponding class labels\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_text, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Vectorize text data using CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "# X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# # Instantiate and train Bernoulli Naive Bayes\n",
    "# bernoulli_nb = BernoulliNB()\n",
    "# bernoulli_nb.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# # Instantiate and train Multinomial Naive Bayes\n",
    "# multinomial_nb = MultinomialNB()\n",
    "# multinomial_nb.fit(X_train_vectorized, y_train)\n",
    "# ```\n",
    "\n",
    "# Choosing between Bernoulli and Multinomial Naive Bayes depends on the type of features, and experimentation or\n",
    "#evaluation on the specific problem is recommended to determine the most suitable classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8f88e4-5ff0-4214-b5d4-3ac4bbb6e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : How does Bernoulli Naive Bayes handle missing values?\n",
    "#Answer.3 : # Handling Missing Values in Bernoulli Naive Bayes :\n",
    "\n",
    "# Bernoulli Naive Bayes assumes that features are binary (0 or 1), and it's not explicitly designed to handle\n",
    "#missing values. However, there are some strategies to deal with missing values when using Bernoulli Naive Bayes:\n",
    "\n",
    "# 1. **Imputation:**\n",
    "#    - Replace missing values with a specific value (e.g., 0 or 1) based on domain knowledge or statistical measures.\n",
    "#    - Imputation can help maintain the binary nature of features.\n",
    "\n",
    "# 2. **Encode Missing Values:**\n",
    "#    - Treat missing values as a separate category by encoding them as a specific value.\n",
    "#    - Assign a placeholder value (e.g., -1) to represent missing values and ensure that the encoding aligns\n",
    "#with the binary nature of features.\n",
    "\n",
    "# Example Code (Handling Missing Values in Bernoulli Naive Bayes):\n",
    "# ```python\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Example data with missing values represented by NaN\n",
    "# X_text_with_missing_values = [\"document1\", \"document2\", ..., \"document_with_missing_values\"]\n",
    "# y_labels = [0, 1, ..., 1]\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_text_with_missing_values, y_labels, test_size=0.2,\n",
    "#random_state=42)\n",
    "\n",
    "# # Vectorize text data using CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "# X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# # Handle missing values using imputation\n",
    "# imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "# X_train_vectorized_imputed = imputer.fit_transform(X_train_vectorized)\n",
    "# X_test_vectorized_imputed = imputer.transform(X_test_vectorized)\n",
    "\n",
    "# # Instantiate and train Bernoulli Naive Bayes\n",
    "# bernoulli_nb = BernoulliNB()\n",
    "# bernoulli_nb.fit(X_train_vectorized_imputed, y_train)\n",
    "# ```\n",
    "\n",
    "# It's important to note that the choice of handling missing values depends on the specific characteristics of \n",
    "#the data and the problem, and experimentation may be needed to determine the most effective strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8945d0-6dde-4f22-8476-0e273ef6b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "#Answer.4 : # Gaussian Naive Bayes for Multi-Class Classification :\n",
    "\n",
    "# Yes, Gaussian Naive Bayes can be used for multi-class classification problems. Unlike the Bernoulli \n",
    "#and Multinomial variants, Gaussian Naive Bayes is suitable for continuous or real-valued features.\n",
    "\n",
    "# Here's how you can use Gaussian Naive Bayes for multi-class classification:\n",
    "\n",
    "# Example Code (Gaussian Naive Bayes for Multi-Class Classification):\n",
    "# ```python\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Load the Iris dataset as an example\n",
    "# iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Instantiate and train Gaussian Naive Bayes\n",
    "# gaussian_nb = GaussianNB()\n",
    "# gaussian_nb.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = gaussian_nb.predict(X_test)\n",
    "\n",
    "# # Evaluate the performance\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# classification_report_result = classification_report(y_test, y_pred)\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(\"Classification Report:\\n\", classification_report_result)\n",
    "# ```\n",
    "\n",
    "# In this example, Gaussian Naive Bayes is used for multi-class classification on the Iris dataset. The \n",
    "#`GaussianNB` class in scikit-learn automatically handles multiple classes.\n",
    "\n",
    "# It's important to note that the choice of Naive Bayes variant depends on the nature of the features in\n",
    "#your dataset, and Gaussian Naive Bayes is suitable for continuous features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ccc296-9a13-47d5-b618-13c62cb99aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question.5 : Assignment:\n",
    "#Data preparation:\n",
    "#Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "#datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "#is spam or not based on several input features.\n",
    "#Implementation:\n",
    "#Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "#scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "#dataset. You should use the default hyperparameters for each classifier.\n",
    "#Results:\n",
    "#Report the following performance metrics for each classifier:\n",
    "#Accuracy\n",
    "#Precision\n",
    "#Recall\n",
    "#F1 score\n",
    "#Discussion:\n",
    "#Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "#the case? Are there any limitations of Naive Bayes that you observed?\n",
    "#Conclusion:\n",
    "#Summarise your findings and provide some suggestions for future work.\n",
    "#Answer.5 : \n",
    "# Bernoulli, Multinomial, and Gaussian Naive Bayes Implementation and Evaluation in Python Comments:\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Spambase dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\",\n",
    "    \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\",\n",
    "    \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n",
    "    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\",\n",
    "    \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n",
    "    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\", \"capital_run_length_total\", \"class\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Define classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Evaluate classifiers using cross-validation\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1').mean()\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate Bernoulli Naive Bayes\n",
    "accuracy_bernoulli, precision_bernoulli, recall_bernoulli, f1_bernoulli = evaluate_classifier(bernoulli_nb, X, y)\n",
    "\n",
    "# Evaluate Multinomial Naive Bayes\n",
    "accuracy_multinomial, precision_multinomial, recall_multinomial, f1_multinomial = evaluate_classifier(multinomial_nb, X, y)\n",
    "\n",
    "# Evaluate Gaussian Naive Bayes\n",
    "accuracy_gaussian, precision_gaussian, recall_gaussian, f1_gaussian = evaluate_classifier(gaussian_nb, X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_bernoulli}\")\n",
    "print(f\"Precision: {precision_bernoulli}\")\n",
    "print(f\"Recall: {recall_bernoulli}\")\n",
    "print(f\"F1 Score: {f1_bernoulli}\")\n",
    "print()\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_multinomial}\")\n",
    "print(f\"Precision: {precision_multinomial}\")\n",
    "print(f\"Recall: {recall_multinomial}\")\n",
    "print(f\"F1 Score: {f1_multinomial}\")\n",
    "print()\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_gaussian}\")\n",
    "print(f\"Precision: {precision_gaussian}\")\n",
    "print(f\"Recall: {recall_gaussian}\")\n",
    "print(f\"F1 Score: {f1_gaussian}\")\n",
    "print()\n",
    "\n",
    "# Discussion:\n",
    "# Discuss the results, compare the performance of different Naive Bayes variants, and highlight any observations.\n",
    "\n",
    "# Conclusion:\n",
    "# Summarize findings, suggest future work, and conclude the analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
