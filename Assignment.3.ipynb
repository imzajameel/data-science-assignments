{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9412e174-28cc-42a8-8e82-01fbe5fe79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are a data scientist working for a healthcare company, and you have been tasked with creating a\n",
    "#decision tree to help identify patients with diabetes based on a set of clinical variables. You have been\n",
    "#given a dataset (diabetes.csv) with the following variables:\n",
    "#1. Pregnancies: Number of times pregnant (integer)\n",
    "#2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (integer)\n",
    "#3. BloodPressure: Diastolic blood pressure (mm Hg) (integer)\n",
    "#4. SkinThickness: Triceps skin fold thickness (mm) (integer)\n",
    "#5. Insulin: 2-Hour serum insulin (mu U/ml) (integer)\n",
    "#6. BMI: Body mass index (weight in kg/(height in m)^2) (float)\n",
    "#7. DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes\n",
    "#based on family history) (float)\n",
    "#8. Age: Age in years (integer)\n",
    "#9. Outcome: Class variable (0 if non-diabetic, 1 if diabetic) (integer)\n",
    "\n",
    "#Question.1:  Import the dataset and examine the variables. Use descriptive statistics and visualizations to\n",
    "#understand the distribution and relationships between the variables.\n",
    "#Answer.1 : \n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'path_to_your_diabetes.csv_file'\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "#print(\"Sample of the dataset:\")\n",
    "#print(df.head())\n",
    "\n",
    "# Descriptive statistics\n",
    "#print(\"\\nDescriptive Statistics:\")\n",
    "#print(df.describe())\n",
    "\n",
    "# Visualize the distribution of each variable\n",
    "#plt.figure(figsize=(12, 8))\n",
    "#df.hist(bins=20)\n",
    "#plt.suptitle(\"Distribution of Variables\")\n",
    "#plt.show()\n",
    "\n",
    "# Visualize relationships between variables using pair plots\n",
    "#sns.pairplot(df, hue='Outcome', diag_kind='kde')\n",
    "#plt.suptitle(\"Pair Plot of Variables with Outcome Colored\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072fceb9-3fda-448c-9246-839fdf149752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : Preprocess the data by cleaning missing values, removing outliers, and transforming categorical\n",
    "#variables into dummy variables if necessary.\n",
    "#Answer.2 : \n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'path_to_your_diabetes.csv_file'\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the number of missing values in each column\n",
    "#print(\"Missing Values:\")\n",
    "#print(df.isnull().sum())\n",
    "\n",
    "# Handling missing values (impute with mean for simplicity)\n",
    "#imputer = SimpleImputer(strategy='mean')\n",
    "#df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display the number of missing values after imputation\n",
    "#print(\"\\nMissing Values after Imputation:\")\n",
    "#print(df_imputed.isnull().sum())\n",
    "\n",
    "# Visualize outliers using box plots\n",
    "#plt.figure(figsize=(12, 8))\n",
    "#sns.boxplot(data=df_imputed)\n",
    "#plt.title(\"Boxplot of Variables to Identify Outliers\")\n",
    "#plt.show()\n",
    "\n",
    "# Remove outliers using z-score (consider adjusting the threshold)\n",
    "#z_scores = (df_imputed - df_imputed.mean()) / df_imputed.std()\n",
    "#df_no_outliers = df_imputed[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Display the shape before and after removing outliers\n",
    "#print(f\"\\nShape before removing outliers: {df_imputed.shape}\")\n",
    "#print(f\"Shape after removing outliers: {df_no_outliers.shape}\")\n",
    "\n",
    "# Transform categorical variables into dummy variables if necessary\n",
    "# (Assuming there are no categorical variables in this dataset)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "#X = df_no_outliers.drop('Outcome', axis=1)\n",
    "#y = df_no_outliers['Outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional but often recommended for decision trees)\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981d02e6-a8b8-4d5f-8af3-8c659c283e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility.\n",
    "#Answer.3 : \n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'path_to_your_diabetes.csv_file'\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "#missing_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "#imputer = SimpleImputer(missing_values=0, strategy='mean')\n",
    "#df[missing_cols] = imputer.fit_transform(df[missing_cols])\n",
    "\n",
    "# Remove outliers (you may choose a different method based on your analysis)\n",
    "# For simplicity, we'll use Z-score to remove outliers in this example\n",
    "#from scipy.stats import zscore\n",
    "#z_scores = zscore(df[missing_cols])\n",
    "#df_no_outliers = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Transform categorical variables into dummy variables (if necessary)\n",
    "# Assuming there are no categorical variables in this dataset\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "#X = df_no_outliers.drop('Outcome', axis=1)\n",
    "#y = df_no_outliers['Outcome']\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional but often recommended for decision tree-based models)\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print information about the processed datasets\n",
    "#print(\"Processed Training Set:\")\n",
    "#print(X_train_scaled.shape, y_train.shape)\n",
    "\n",
    "#print(\"\\nProcessed Test Set:\")\n",
    "#print(X_test_scaled.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1898c82c-4d2e-4224-86bb-990091ac45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set.\n",
    "#Use cross-validation to optimize the hyperparameters and avoid overfitting.\n",
    "#Answer.4 : \n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'path_to_your_diabetes.csv_file'\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Handling missing values\n",
    "#imputer = SimpleImputer(strategy='mean')\n",
    "#df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Removing outliers (assuming 'Outcome' is the target variable)\n",
    "#Q1 = df_imputed.quantile(0.25)\n",
    "#Q3 = df_imputed.quantile(0.75)\n",
    "#IQR = Q3 - Q1\n",
    "#df_no_outliers = df_imputed[~((df_imputed < (Q1 - 1.5 * IQR)) | (df_imputed > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Transforming categorical variables into dummy variables (if necessary)\n",
    "# (Note: This step is not necessary for the given dataset, as all variables seem to be numeric)\n",
    "\n",
    "# Step 3: Split the dataset into a training set and a test set\n",
    "#X = df_no_outliers.drop('Outcome', axis=1)\n",
    "#y = df_no_outliers['Outcome']\n",
    "\n",
    "# Use a random seed for reproducibility\n",
    "#random_seed = 42\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Step 4: Train a decision tree model and perform cross-validation\n",
    "# We'll use a pipeline to scale features and train the decision tree\n",
    "#model = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=random_seed))\n",
    "\n",
    "# Perform cross-validation to optimize hyperparameters\n",
    "#cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "#mean_cv_accuracy = cv_scores.mean()\n",
    "\n",
    "# Fit the model on the training set\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Print cross-validation results\n",
    "#print(f\"Cross-Validation Mean Accuracy: {mean_cv_accuracy:.2f}\")\n",
    "\n",
    "# Optionally, you can print other cross-validation metrics or hyperparameter tuning results.\n",
    "\n",
    "# Note: Depending on the dataset, you might want to fine-tune hyperparameters further or explore other decision tree\n",
    "#algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11cd776-5f96-49d8-ae32-da584093cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : Evaluate the performance of the decision tree model on the test set using metrics such as accuracy,\n",
    "#precision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results.\n",
    "#Answer.5 : \n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 5: Evaluate the performance on the test set\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#precision = precision_score(y_test, y_pred)\n",
    "#recall = recall_score(y_test, y_pred)\n",
    "#f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#print(f\"Accuracy: {accuracy:.2f}\")\n",
    "#print(f\"Precision: {precision:.2f}\")\n",
    "#print(f\"Recall: {recall:.2f}\")\n",
    "#print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "#conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(conf_matrix)\n",
    "\n",
    "# Plot ROC curve\n",
    "#fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "#plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "#plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd71ddd3-5c76-4e0a-b03b-0f8700c6f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important\n",
    "#variables and their thresholds. Use domain knowledge and common sense to explain the patterns and\n",
    "#trends.\n",
    "#Answer.6 : \n",
    "# Interpret the decision tree\n",
    "#feature_importances = model.named_steps['decisiontreeclassifier'].feature_importances_\n",
    "#important_features = X.columns[np.argsort(feature_importances)[::-1]]\n",
    "#thresholds = model.named_steps['decisiontreeclassifier'].tree_.threshold\n",
    "\n",
    "#print(\"\\nDecision Tree Interpretation:\")\n",
    "#print(f\"Important Features: {list(important_features)}\")\n",
    "\n",
    "# Display the first few levels of the decision tree\n",
    "#tree_levels = 3\n",
    "#tree_structure = model.named_steps['decisiontreeclassifier'].tree_\n",
    "\n",
    "#def print_tree(node, depth):\n",
    "    #indent = \"  \" * depth\n",
    "    #if tree_structure.feature[node] != tree_structure.TREE_UNDEFINED:\n",
    "        #name = important_features[tree_structure.feature[node]]\n",
    "       # threshold = thresholds[node]\n",
    "       # print(f\"{indent}if {name} <= {threshold:.2f}:\")\n",
    "        #print_tree(tree_structure.children_left[node], depth + 1)\n",
    "        #print(f\"{indent}else:\")\n",
    "       # print_tree(tree_structure.children_right[node], depth + 1)\n",
    "    #else:\n",
    "       # print(f\"{indent}class {tree_structure.value[node].argmax()}\")\n",
    "\n",
    "#print_tree(0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f645f20-12a2-45b3-8c6c-a43623dd76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : Validate the decision tree model by applying it to new data or testing its robustness to changes in the\n",
    "#dataset or the environment. Use sensitivity analysis and scenario testing to explore the uncertainty and\n",
    "#risks.\n",
    "#Answer.7 : \n",
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "# Apply the decision tree model to new data (e.g., validation set)\n",
    "# Note: Use a separate dataset that the model has not seen during training or testing\n",
    "#X_new_data = ...  # Provide the new data features\n",
    "#y_new_data_true = ...  # Provide the true labels for the new data\n",
    "\n",
    "# Predict labels for the new data\n",
    "#y_new_data_pred = model.predict(X_new_data)\n",
    "\n",
    "# Evaluate performance on the new data\n",
    "#print(\"\\nPerformance on New Data:\")\n",
    "#print(classification_report(y_new_data_true, y_new_data_pred))\n",
    "\n",
    "# Sensitivity Analysis: Evaluate model performance under changes in input features\n",
    "# For example, perturb one feature at a time and observe the impact on predictions\n",
    "#sensitivity_feature = 'Glucose'  # Specify the feature for sensitivity analysis\n",
    "#perturbed_data = X_new_data.copy()\n",
    "#perturbed_data[sensitivity_feature] += 10  # Perturb the feature by a small amount\n",
    "\n",
    "#y_perturbed_pred = model.predict(perturbed_data)\n",
    "\n",
    "# Evaluate performance on perturbed data\n",
    "#print(\"\\nSensitivity Analysis:\")\n",
    "#print(f\"Perturbed Feature: {sensitivity_feature}\")\n",
    "#print(classification_report(y_new_data_true, y_perturbed_pred))\n",
    "\n",
    "# Scenario Testing: Explore model behavior in specific scenarios\n",
    "# Simulate scenarios by changing relevant features\n",
    "#scenario_data = X_new_data.copy()\n",
    "#scenario_data['BMI'] += 5  # Simulate a scenario with higher BMI\n",
    "\n",
    "#y_scenario_pred = model.predict(scenario_data)\n",
    "\n",
    "# Evaluate performance on scenario data\n",
    "#print(\"\\nScenario Testing:\")\n",
    "#print(\"Scenario: Higher BMI\")\n",
    "#print(classification_report(y_new_data_true, y_scenario_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
