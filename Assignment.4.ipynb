{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860251f8-0eba-41c9-ad0d-2a92aaa1d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.1 :  What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "#algorithms?\n",
    "#Answer.1 : \n",
    "# Relationship between Polynomial Functions and Kernel Functions in Machine Learning\n",
    "\n",
    "# Polynomial Functions:\n",
    "# Polynomial functions are mathematical expressions of the form f(x) = a_n * x^n + a_(n-1) * x^(n-1) + ... + a_1 * x \n",
    "#+ a_0.\n",
    "# They involve powers of the input variable 'x' with coefficients 'a_i', and 'n' represents the degree of the polynomial.\n",
    "# In machine learning, polynomial functions are often used for feature transformation and representation.\n",
    "\n",
    "# Kernel Functions:\n",
    "# Kernel functions are crucial in machine learning, especially in algorithms like Support Vector Machines (SVM).\n",
    "# They compute the similarity or inner product between input vectors without explicitly transforming them into a \n",
    "#higher-dimensional space.\n",
    "# This allows algorithms to operate efficiently in high-dimensional spaces without explicitly computing the \n",
    "#transformed feature vectors.\n",
    "\n",
    "# Relationship:\n",
    "# Polynomial kernel functions are a type of kernel used in SVMs, specifically designed for polynomial feature\n",
    "#transformations.\n",
    "# The polynomial kernel function is defined as K(x, y) = (x * y + c)^d, where 'd' is the degree of the polynomial,\n",
    "#and 'c' is a constant term.\n",
    "# This kernel captures pairwise interactions between features, enabling SVMs to learn complex decision boundaries in\n",
    "#the transformed space.\n",
    "\n",
    "# In scikit-learn, the polynomial kernel is commonly used in SVMs, and it is specified by setting the 'kernel' parameter\n",
    "#to 'poly'.\n",
    "# Additionally, parameters like 'degree' and 'coef0' can be tuned to control the degree of the polynomial and the \n",
    "#constant term.\n",
    "\n",
    "# Example:\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a simple dataset\n",
    "#X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "#svm_poly = SVC(kernel='poly', degree=3, coef0=1, C=1)\n",
    "#svm_poly.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfdbe04-fd3a-4bfc-bab9-c0fda178d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "#Answer.2 : \n",
    "# Implementing SVM with Polynomial Kernel in Python using Scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "#X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "# Set the 'kernel' parameter to 'poly' to specify the polynomial kernel\n",
    "# Specify the 'degree' parameter to set the degree of the polynomial\n",
    "# 'C' is a regularization parameter controlling the trade-off between smooth decision boundary and classifying\n",
    "#training points correctly\n",
    "#svm_poly = SVC(kernel='poly', degree=3, C=1)\n",
    "\n",
    "# Train the SVM classifier on the training set\n",
    "#svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "#y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Note: Adjust the parameters (degree, C) based on the characteristics of your data and the complexity of the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9624dec7-59eb-4358-97e3-c34933925f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "#Answer.3 : \n",
    "# Impact of Epsilon on the Number of Support Vectors in SVR\n",
    "\n",
    "# Support Vector Regression (SVR) is a machine learning algorithm for regression tasks.\n",
    "# Epsilon (Îµ) is a hyperparameter in SVR that controls the width of the margin or the acceptable deviation of \n",
    "#predictions from the true values.\n",
    "\n",
    "# Scikit-learn's SVR implementation has a parameter called 'epsilon' (epsilon-SVR).\n",
    "# The 'epsilon' parameter represents the size of the epsilon-tube within which no penalty is associated in the\n",
    "#training loss function.\n",
    "\n",
    "# Let's explore how increasing the value of epsilon affects the number of support vectors.\n",
    "\n",
    "#from sklearn.svm import SVR\n",
    "#from sklearn.datasets import make_regression\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "#X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Varying values of epsilon\n",
    "#epsilon_values = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "# Plot the SVR models with different epsilon values\n",
    "#plt.figure(figsize=(12, 8))\n",
    "\n",
    "#for epsilon in epsilon_values:\n",
    "    # Create SVR model with the specified epsilon\n",
    "    #svr_model = SVR(epsilon=epsilon)\n",
    "    \n",
    "    # Fit the model on the dataset\n",
    "    #svr_model.fit(X, y)\n",
    "    \n",
    "    # Plot the regression function\n",
    "    #plt.plot(X, svr_model.predict(X), label=f'Epsilon={epsilon}')\n",
    "\n",
    "# Scatter plot of the data points\n",
    "#plt.scatter(X, y, label='Data Points', color='black')\n",
    "\n",
    "# Customize the plot\n",
    "#plt.title('Impact of Epsilon on SVR Regression Function')\n",
    "#plt.xlabel('Feature')\n",
    "#plt.ylabel('Target')\n",
    "#plt.legend()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ad6b8c-ef10-45ea-8c7a-769197f1744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "#affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "#and provide examples of when you might want to increase or decrease its value?\n",
    "#Answer.4 : \n",
    "# Support Vector Regression (SVR) Parameters and Their Impact\n",
    "\n",
    "# Kernel Function:\n",
    "# - Explanation: Determines the type of transformation applied to input features.\n",
    "# - Example: \n",
    "#   - Use linear kernel (kernel='linear') for linear relationships.\n",
    "#   - Use polynomial kernel (kernel='poly') for non-linear relationships.\n",
    "#   - Use RBF kernel (kernel='rbf') for complex non-linear relationships.\n",
    "\n",
    "# C Parameter:\n",
    "# - Explanation: Controls the trade-off between achieving low training error and a smooth decision boundary.\n",
    "# - Example: \n",
    "#   - Use smaller C (C=0.1) for a smoother model, allowing some training errors.\n",
    "#   - Use larger C (C=10) for a more complex model, minimizing training errors.\n",
    "\n",
    "# Epsilon Parameter:\n",
    "# - Explanation: Defines the size of the epsilon-tube, controlling the width of the margin around predicted values.\n",
    "# - Example: \n",
    "#   - Use smaller epsilon (epsilon=0.1) to penalize even small deviations from true values.\n",
    "#   - Use larger epsilon (epsilon=1.0) to allow larger deviations, prioritizing a smoother regression function.\n",
    "\n",
    "# Gamma Parameter:\n",
    "# - Explanation: Defines the width of the RBF kernel, affecting the influence of individual data points.\n",
    "# - Example: \n",
    "#   - Use smaller gamma (gamma=0.01) for a wider curve, more global influence, and smoother regression.\n",
    "#   - Use larger gamma (gamma=1.0) for a narrower curve, more localized influence, and capturing intricate patterns.\n",
    "\n",
    "# Example Usage (Note: Adjust these values based on your data characteristics):\n",
    "#from sklearn.svm import SVR\n",
    "#from sklearn.datasets import make_regression\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a regression dataset\n",
    "#X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVR model with specified parameters\n",
    "#svr_model = SVR(kernel='rbf', C=1, epsilon=0.1, gamma=0.1)\n",
    "\n",
    "# Train the SVR model\n",
    "#svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "#y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "#mse = mean_squared_error(y_test, y_pred)\n",
    "#print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70be14-f1d1-4716-a58e-85780c75f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : Assignment:\n",
    "# Import the necessary libraries and load the dataset\n",
    "# Split the dataset into training and testing set\n",
    "# Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "# hse the trained classifier to predict the labels of the testing data\n",
    "# Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "#precision, recall, F1-score)\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "#improve its performance\n",
    "# Train the tuned classifier on the entire dataset\n",
    "# Save the trained classifier to a file for future use.\n",
    "#Answer.5 : \n",
    "# Import necessary libraries\n",
    "#from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "#from sklearn import datasets\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#import joblib\n",
    "\n",
    "# Load the dataset\n",
    "#iris = datasets.load_iris()\n",
    "#X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling in this example)\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "#svc_classifier = SVC()\n",
    "#svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "#y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Optionally, evaluate using other metrics like precision, recall, F1-score\n",
    "#print(\"Classification Report:\")\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "#param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "#grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "#grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "#best_params = grid_search.best_params_\n",
    "#print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "#svc_tuned_classifier = SVC(**best_params)\n",
    "#svc_tuned_classifier.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "#joblib.dump(svc_tuned_classifier, 'svc_tuned_classifier.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
